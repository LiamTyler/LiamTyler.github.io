<!DOCTYPE HTML>
<html>
  <head>
    <meta charset='utf-8'>
    <title>Liam Tyler</title>
    <link rel='stylesheet' type='text/css' href='/styles/styles.css'>
    <link href="/styles/prism.css" rel="stylesheet" />
    <script src='/scripts/jquery-3.2.1.js'></script>
    <link rel="icon" href="/images/favicon.ico" type="image/x-icon"/>

<!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-145372343-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-145372343-1');
    </script>
  </head>
  <body>
    <div class="navbar">
      <nav>
        <ul>
          <div class="floating">
            <li id="LT-header">Liam Tyler<p>Software Developer</p></li>
            <li><a href="/index.html">Home</a></li>
            <li><a href="/about/index.html">About</a></li>
            <li><a href="/portfolio/index.html">Portfolio</a></li>
            <li><a href="/work/index.html">Work & Research</a></li>
          </div>
        </ul>
      </nav>
    </div>
    <script src="/scripts/prism.js"></script>
    <div id="projects">
      <h1 class="centerText">Progression</h1>
      <hr>

      <h2>Description</h2>
      <p>Progression is a 3D game engine written in C++ for Linux and Windows. It originally started out as an simple OpenGL renderer for class projects, but it now features a Vulkan renderer, Lua scripting, skeletal animation, asset serialization, and more. It is my largest project to date, even though I keep rewriting it as I learn more and gain more experience with C++ and graphics.</p>

      <h2>Code</h2>
      <p>Code from this project can be found on Github <a href="https://github.com/LiamTyler/Progression">here</a></p>

      <h2>Features</h2>
      <h5>Note: for SSAO, deferred rendering, shadow mapping, and normal mapping I partnered up with <a href="https://sites.google.com/umn.edu/zachsportfolio/home?fbclid=IwAR1FnQC03IqJdBMhxeNZn87i8_SSe2-Nqb-QXauz415a9FeM0b06g5vd0L0">Zach Chavis</a> for them as part of a project</h5>
      
      <h3>Screen Space Ambient Occlusion (SSAO)</h3>
      <p>We took the approach described in <a href="https://learnopengl.com/Advanced-Lighting/SSAO">this</a> tutorial. This method randomly samples many nearby points for each pixel, and return which fraction of those samples were occluded by other nearby geometry. This can be seen in the following image:</p>
      <div class="centerText">
        <img class="capWidth" src="ssao_hemisphere.png">
        <p>Two examples of the SSAO hemisphere for two points. Image from the tutorial above.</p>
      </div>
      <br>
      <p>To sample random points within this hemisphere surrounding each pixel, we need to use random offsets. Since it would be impractical to generate or store random offsets for each possible orientation of the hemisphere, we instead create a "kernel", which is a buffer of random offsets in the tangent space. These offsets are scaled so that most of them are closer to the center: </p>
      <pre><code class="language-c++">
const float numSamples = 32;
std::vector< glm::vec4 > kernel( numSamples );
for ( int i = 0; i < (int) numSamples; ++i )
{
    glm::vec3 sample( randomFloat() * 2 - 1, randomFloat() * 2 - 1, randomFloat() );
    sample      = randomFloat() * glm::normalize( sample );
    float scale = lerp( 0.1, 1.0, ( i / numSamples ) * ( i / numSamples );
    kernel[i]   = glm::vec4( scale * sample, 0 );
}
      </pre></code>
      <p>We also do not want every single point to have the example same random samples, since this might cause some structure or artifacts in the final image, and require us to use more samples to get rid of these issues. To accomplish this, the hemisphere is randomly rotated before sampling. No rotation matrix is actually used, but instead random tangent vectors are calculated. Again it would be a lot more memory to store a tangent for every pixel, so we only create a 4x4 texture of tangents, and repeat them across the image:</p>
      <pre><code class="language-c++">
std::vector< glm::vec4 > noise( 16 );
for ( int i = 0; i < 16; ++i )
{
    noise[i] = glm::vec4( randomFloat() * 2 - 1, randomFloat() * 2 - 1, 0, 0 );
}
      </pre></code>
      <p>After uploading the kernel and tangent texture, the actual SSAO shader needs to be run. First we need to do Gramm-Schmidt to generate the orthonormal basis:</p>
      <pre><code class="language-glsl">
vec3 randomVec = texture( ssaoNoise, noiseUV ).xyz;
vec3 T         = normalize( randomVec - N * dot( N, randomVec ) );
vec3 B         = cross( N, T );
mat3 TBN       = mat3( T, B, N );
      </code></pre>
      <p>One interesting note here is that N is in view space, and to my understanding the tutorial we followed chose z = 0 for the random vectors so that the random vector would never be parallel to N. If they were paralle, then T would be normalizing the vector ( 0, 0, 0 ) and our basis would be defficient. View space vectors can have z <= 0 though, so I don't understand how we can guarantee an orthonormal basis here, or why view space is advantageous. It looks fine though, so we didn't change anything with it. The next step is actually sample the points, and add up how many of them are occluded by the actual saved geometry:</p>
      <pre><code class="language-glsl">
float SCALE_RADIUS = 0.5;
float occlusion = 0;
for ( int i = 0; i < 32; ++i )
{
    vec3 offsetPos  = fragPos + TBN * uboSSAOKernel.samples[i].xyz * SCALE_RADIUS;
    
    vec4 projCoords = matrices.P * vec4( offsetPos, 1 );
    projCoords.xyz /= projCoords.w;
    projCoords.xy   = 0.5 * projCoords.xy + vec2( 0.5 );
    projCoords.y    = 1 - projCoords.y; // since current Vulkan viewport is inverted
    
    float offsetDepth = ( matrices.V * texture( worldPositions, projCoords.xy ) ).z;
    float rangeCheck  = smoothstep(0.0f, 1.0f, SCALE_RADIUS / abs(fragPos.z - offsetDepth));
    occlusion         += (offsetDepth >= offsetPos.z + BIAS ? 1.0f : 0.0f) * rangeCheck;
}
      </code></pre>
      <p>Another point of interest here is the "rangeCheck". An object that is occluding another won't really affect the amount of ambient light that point should receive the further away the occluder gets. As a result we want to diminish the effect of those objects. Without it you would see a dark halo surrounding the edges of every object that occludes another one. If we look at the result now however we see some problems:</p>
      <div class="centerText">
        <img class="capWidth" src="sponza_AO_no_blur.png">
        <p>The current ambient occlusion texture. The repeated pattern of the 4x4 noise texture is clearly visible.</p>
      </div>
      <br>
      <p>To fix this is pretty easy, and we just run a blur filter of this image to produce the new ambient occlusion texture and then in the final lighting calculations:</p>
      <div class="centerText">
        <img class="capWidth" src="sponza_AO_only.png">
        <p>The ambient occlusion texture.</p>
        <img class="capWidth" src="sponza_no_AO.jpg">
        <img class="capWidth" src="sponza_with_AO.jpg">
        <p>Sponza without SSAO (top) and Sponza with SSAO (bottom).</p>
      </div>
      
      <br>
      <h3>Deferred Rendering</h3>
      <p>While I had written a tiled-deferred renderer in OpenGL, current Vulkan renderer is just regular deferred rendering without even light volumes currently. Tiled deferred or F+ is on the docket. Only just started trying to consider the GBuffer size and reduce it, but the current GBuffer looks like this:</p>
      <ul>
        <li><strong>Positions: </strong>RGBA32_FLOAT</li>
        <li><strong>Normals: </strong>RGBA8_UNORM (compressed using 24-bit octahedron encoding described in <a href="http://jcgt.org/published/0003/02/01/paper.pdf">this</a> paper)</li>
        <li><strong>Diffuse and Specular Colors: </strong>RGBA16_UINT (upper 8 bits of RGB are diffuse color, lower 8 are specular, and the alpha component is the 16 bit specular exponent)</li>
        <li><strong>Ambient Occlusion: </strong>R8_UNORM</li>
        <li><strong>Depth: </strong>32 bit</li>
      </ul>
      <p>While positions and normals do not need the alpha channel, my GPU does not support RGB textures with optimal tiling. Initially normals were RGBA32_FLOAT, the diffuse and specular were RGBA_UNORM and RGBA16_FLOAT respectively. With each change to the gbuffer we saw the following timings:</p>
      <div class="centerText">
      <img class="capWidth" src="sponza_timings.png">
      <p>Sponza timings with 5 lights in 1080p on my RTX 2080. Yes, the SSAO pass isn't the greatest currently.</p>
      </div>
      <p>Given the speed up from non-trivial normal encoding, I thought I would see the same thing for packing the diffuse and specular color into one 16-bit RGBA texture. On average however, it is the same as with using two textures. Since it saves 32 bits per pixel however, I kept it. I am currently trying to figure out if 16 bit positions are okay, and play around with what I could put in the alpha channel instead of it being unused. I also want to try getting rid of the position buffer altogether and just recover it via the depth buffer.</p>
      
      <br>
      <h3>Directional Light Shadow Mapping</h3>
      <p>Currently the directional light is the only kind of light that can have a shadow map, and it defaults to a 4K map. It is a regular implementation of basic shadow mapping where the scene is rendered once from the perspective of the light. This involves creating a view matrix pointing in the direction of the light, and an orthographic project matrix. During the regular lighting pass, each pixel is transformed into that light's normalized device coordinate space using those view and projection matrices. This lets you look up the compare the pixel's distance from the light to the value saved in the shadow map. If the saved value is closer to the camera than the pixel, that means the pixel is occluded. This can be seen in the following code:</p>
      <pre><code class="language-glsl">
float ShadowAmount( in const vec4 posInLightSpace, in sampler2D shadowMap ) {
    vec3 ndc             = posInLightSpace.xyz / posInLightSpace.w;
    vec3 projCoords      = .5 * ndc + vec3( .5 );
    projCoords.y         = 1 - projCoords.y; // Account for flip in projection matrix
    float currentDepth   = ndc.z;
    
    if ( currentDepth > 1 )
        return 0;
    
    vec2 dUV = 1.0 / textureSize( shadowMap, 0 );
    float totalShadowStrength = 0;
    for ( int r = -1; r <= 1; ++r ) {
        for ( int c = -1; c <= 1; ++c ) {
            vec2 coords = projCoords.xy + vec2( c * dUV.x, r * dUV.y );
            float d = texture( shadowMap, coords ).r;
            if ( d < currentDepth )
                totalShadowStrength += 1.0;
        }
    }
    
    return totalShadowStrength / 9;
}
      </code></pre>
      <p>A few more interesting aspects and related thoughts of this are:</p>
      <ol>
        <li>While a directional light doesn't really have a position or bounds, you have to define a non-infinite frustum with the view and projection matrices. In fact the smaller the frustum the more texels per meter in world space you will have and the better your results will look. Unfortunately there is no automatic calculation of this frustum curently in the engine. In the video further down this page, the moving light is controlled in a Lua script, which is pretty annoying. Automatic frustum calculation is on this TODO list.</li>
        <li>Normally you have to add a bias to avoid shadow acne artifacts. A convenient thing about Vulkan is that it lets you easily set constant and slope bias factors. I use those, which means that the values saved into the shadow map are already pre-biased, and there is no need to change them in the lighting pass shader.</li>
        <li>Shadow passes are the reason that the VBOs of models are non-interleaved. I haven't actually profiled it, but I suspected that in a depth-only pass where you only need vertex positions, it would be better cache performance if all of the positions were packed tightly together.</li>
        <li>Currently I am doing PCF filtering by averaging a 3x3 kernel. Even with a 4K map and tight frustum bounds, the blurines from this is easily near the camera. As a result cascaded shadow maps are also on the TODO list.</li>
      </ol>
      
      <br>
      <h3>Normal Mapping</h3>
      <p>The VBO stores the normals and tangents, but not the bitangents since those are calculated in the shader. The TBN matrix which transforms vectors from tangents space to world space is calculated with the following pseudocode:</p>
      <pre><code class="language-glsl">
vec3 worldT = normalize( modelMatrix * inTangent );
vec3 worldN = normalize( inverse( transpose( modelMatrix ) ) * inNormal );
vec3 worldB = cross( worldN, worldT );
TBN         = mat3( worldT, worldB, worldN );
      </code></pre>

      <p>All the blogs and tutorials we saw online multiply both the tangent and normal by just the model matrix, but we are pretty sure that since the normal does not lie on the geometry surface, just multiplying by the model matrix would skew the result for non-uniform scaling. The tangent and bitangent lie along the surface though and do not need the inverse transpose. We then just look up the normal map value, normalize to [-1, 1] and apply the matrix:</p>
      <pre><code class="language-glsl">
vec3 n = texture( normalMap, texCoord );
n      = normalize( n * 2 - 1 );
n      = normalize( TBN * n );
      </code></pre>
      
      <br>
      <h3>GPU Profiling</h3>
      <p>I had wanted to implement a simple system for timing Vulkan commands for a while. I finally got around to doing it when I started considering different GBuffer formats and packings, since that was the only detailed way to tell how each change was impacting performance. The system I implemented uses the built in "vkCmdWriteTimestamp". The user can insert timestamps into the command buffer and mark them with an name as well. By default the timings won't be printed out unless the user manually looks them up. The default is to wait until the application is exiting to print out all of the averages for the timestamps that start with the same prefix, such as "Frame_Start" and "Frame_End". An example usage of this in my main draw function is shown below:</p>
      
      <pre><code class="language-c++">
cmdBuf.BeginRecording();
PG_PROFILE_RESET( cmdBuf );

PG_PROFILE_TIMESTAMP( cmdBuf, "Frame_Start" );
ShadowPass( scene, cmdBuf );
GBufferPass( scene, cmdBuf );
SSAOPass( scene, cmdBuf );
DeferredLightingPass( scene, cmdBuf );
PostProcessPass( scene, cmdBuf, swapChainImageIndex );
PG_PROFILE_TIMESTAMP( cmdBuf, "Frame_End" );

PG_PROFILE_GET_RESULTS();
      </code></pre>
      <div class="centerText">
        <img class="capWidth" src="gpu_profiling_sponza_example.jpg">
        <p>Output from a normal run of Sponza in my engine</p>
      </div>
      
      <p>The reason everything is a macro is because in my engine I have multiple build configurations. The slowest is "Debug" and the fastest is "Ship". In the Ship build, I want everything to be as fast as possible, so I turn off logging, profiling, resource naming, etc. Macros provide an easy way for me to disable the functions entirely for specific builds, ensuring no performance hit.</p>
      
      
      <br>
      <h3>RenderDoc Integration</h3>
      <p>A small feature in my engine that has helped me debug a lot faster was giving resources and regions names that are visible in RenderDoc captures as seen below:</p>
      <div class="centerText">
        <img class="capWidth" src="renderdoc_regions_comparison.jpg">
        <p>How the regions and draw commands used to look (left), and after the new feature (right)</p>
        <br>
        <img class="capWidth" src="renderdoc_resources_comparison.jpg">
        <p>How the resource inspector used to look (left), and after the new feature (right)</p>
      </div>
      <p>This was a small feature, but it already has saved me lots of time being able to find the exact resources and draw calls I am interested in. Names and regions are registered using the VK_EXT_debug_marker extension. In my graphics api that sits ontop of vulkan, each resource has an optional last parameter during its creation to specify the name. Just like the GPU profiler, the API for naming is a series of macros. This is so that in Ship builds these names disappear for max performance.</p>
      
      <h3>Skeletal Animation, Lua Scripting, Asset serialization, Entity Component System</h3>
      <p>There are a lot more cool features unrelated to rendering in my engine, especially the ones listed above. I am still touching up on these areas however, and will add a more detailed explanation of how these work in my engine another time.</p>

      <br>
      <h2>Videos</h2>
      <iframe width="560" height="315" src="https://www.youtube.com/embed/Du04-9DlrCA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <p class="centerText">A short demo highlighting some of the more recent rendering features</p>
      <br>
    
      <h3>Special Thanks</h3>
      <p>I want to give a special thanks to Moses Adeagbo for inspiring me to start this in the first place and helping me a lot with OpenGL in the beginning. Stephen Guy for putting in way more time than he had to as a professor helping me understand many of these concepts. <a href="https://sites.google.com/umn.edu/zachsportfolio/home?fbclid=IwAR1FnQC03IqJdBMhxeNZn87i8_SSe2-Nqb-QXauz415a9FeM0b06g5vd0L0">Zach Chavis</a> for being the only person at the UMN that wanted to partner up with me to do cool graphics in Vulkan when we did the first round of SSAO, deferred rendering, normal mapping, and directional shadow mapping.</p>
      <h3>External Libraries Used</h3>
      <ul>
        <li>Vulkan</li>
        <li>EnTT</li>
        <li>Sol2</li>
        <li>Lua</li>
        <li>Assimp</li>
        <li>stb_image</li>
        <li>GLFW</li>
        <li>LZ4</li>
        <li>SPIRV-Reflect</li>
        <li>GLM</li>
        <li>rapidjson</li>
        <li>meshoptimizer</li>
        <li>cpptoml</li>
        <li>MemoryMapped by Stephan Brumme</li>
      </ul>
    
    </div>
    <script src='/scripts/code_snippet_formatter.js'></script>
  </body>
</html>
