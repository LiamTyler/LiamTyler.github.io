[{"content":" Render Graphs, or \u0026ldquo;Task Graphs,\u0026rdquo; rapidly became the standard in the industry ever since Yuriy O\u0026rsquo;Donnell\u0026rsquo;s 2017 GDC presentation on Frostbite\u0026rsquo;s FrameGraph. They provide many benefits, like handling all of the synchronization barriers and resource transitions, reordering tasks for increased performance, reducing memory usage via resource aliasing, and the list goes on for miles. They’re nearly mandatory for large projects these days, given how verbose and error-prone these things would be if done manually with modern APIs like Vulkan and DX12.\nThis article isn’t to explain their benefits, or how they work, or to discuss their designs from a high level; there are plenty of great resources out there already that do that. Rather, this article is to help fill a smaller niche that I don’t see as many people talk about: what are some concrete examples of task graphs for smaller projects, like hobby game engines? It feels like all too often articles about task graphs immediately dive into topics like acyclic graphs, pruning unreferenced render passes, and parallel command recording on multiple queues. I\u0026rsquo;d argue, however, that if you\u0026rsquo;re writing your first task graph system, you absolutely shouldn\u0026rsquo;t start with any of those things. At least for your personal projects, you were probably never bothered by suboptimal task ordering, had no unreferenced render passes, and had a lot of it single-threaded already. Even professionally, I still don\u0026rsquo;t think it makes sense to immediately write those features unless you have a huge project that has been optimized a lot.\nIn this article, I\u0026rsquo;ll describe the task graph I made for my hobby engine Progression. It keeps things simpler by not adding a lot of commonly talked about features: it doesn\u0026rsquo;t reorder tasks at all, doesn\u0026rsquo;t do any task pruning, doesn\u0026rsquo;t rebuild every frame, and only uses one queue. Instead, it just focuses on the core pain points introduced by modern APIs like Vulkan: synchronization, resource transitions, and resource allocation. Out of all the fancier features, I only chose one to start with: resource memory aliasing. I might build on this in the future, of course, but I think it\u0026rsquo;s already tremendously helpful as-is, and worth sharing. All of the code for this can be found here.\nBuilding The task graph in Progression has two steps: building and compiling. The build step is where all of the passes are defined, all of their inputs and outputs, the resource formats, sizes, etc. I think it\u0026rsquo;s simplest to just show a real example of how it\u0026rsquo;s used and walk through it from there:\nTaskGraphBuilder builder; ComputeTaskBuilder* cTask = builder.AddComputeTask( \u0026#34;FrustumCullMeshes\u0026#34; ); TGBBufferRef indirectCountBuff = cTask-\u0026gt;AddBufferOutput( \u0026#34;indirectCountBuff\u0026#34;, VMA_MEMORY_USAGE_AUTO_PREFER_DEVICE, sizeof( u32 ), 0 ); TGBBufferRef indirectDrawBuff = cTask-\u0026gt;AddBufferOutput( \u0026#34;indirectDrawBuff\u0026#34;, VMA_MEMORY_USAGE_AUTO_PREFER_DEVICE, sizeof( GpuData::MeshletDrawCommand ) * MAX_MESHES_PER_FRAME ); cTask-\u0026gt;SetFunction( ComputeFrustumCullMeshes ); GraphicsTaskBuilder* gTask = builder.AddGraphicsTask( \u0026#34;DrawMeshes\u0026#34; ); gTask-\u0026gt;AddBufferInput( indirectCountBuff, BufferUsage::INDIRECT ); gTask-\u0026gt;AddBufferInput( indirectDrawBuff, BufferUsage::INDIRECT ); TGBTextureRef litOutput = gTask-\u0026gt;AddColorAttachment( \u0026#34;litOutput\u0026#34;, PixelFormat::R16_G16_B16_A16_FLOAT, SIZE_SCENE(), SIZE_SCENE() ); TGBTextureRef sceneDepth = gTask-\u0026gt;AddDepthAttachment( \u0026#34;sceneDepth\u0026#34;, PixelFormat::DEPTH_32_FLOAT, SIZE_SCENE(), SIZE_SCENE(), 1.0f ); gTask-\u0026gt;SetFunction( MeshDrawFunc ); ... First, there is the TaskGraphBuilder object. It\u0026rsquo;s responsible for creating the list of all the tasks and resources defined. You\u0026rsquo;ll notice most of the builder-specific objects either have the postfix Builder or the prefix TGB (for task graph builder). Four possible task types that can be created: compute, graphics, transfer, and present. Each task inherits from a base TaskBuilder. This is mainly so that the TaskGraphBuilder can hold a single list of pointers to all the tasks, and it also holds some basic info about each task, like the name, type, and index into the builder\u0026rsquo;s list. In the above example, a compute task is first defined, which we can dive into next.\nCompute Tasks Compute tasks need support for both buffers and textures for either input or output being added. Graphics tasks also need this functionality, so there is an intermediate builder class that they both inherit from:\nclass PipelineTaskBuilder : public TaskBuilder { public: PipelineTaskBuilder( TaskGraphBuilder* inBuilder, u16 taskIndex, TaskType taskType, string_view inName ); TGBTextureRef AddTextureOutput( string_view name, PixelFormat format, const vec4\u0026amp; clearColor, u32 width, u32 height, u32 depth = 1, u32 arrayLayers = 1, u32 mipLevels = 1 ); TGBTextureRef AddTextureOutput( string_view name, PixelFormat format, u32 width, u32 height, u32 depth = 1, u32 arrayLayers = 1, u32 mipLevels = 1 ); void AddTextureOutput( TGBTextureRef\u0026amp; texture ); void AddTextureInput( TGBTextureRef\u0026amp; texture ); TGBBufferRef AddBufferOutput( string_view name, VmaMemoryUsage memoryUsage, size_t size, u32 clearVal ); TGBBufferRef AddBufferOutput( string_view name, VmaMemoryUsage memoryUsage, size_t size ); void AddBufferOutput( TGBBufferRef\u0026amp; buffer ); void AddBufferInput( TGBBufferRef\u0026amp; buffer, BufferUsage usageForThisTask = BufferUsage::STORAGE ); vector\u0026lt;TGBBufferTaskInfo\u0026gt; buffers; vector\u0026lt;TGBTextureTaskInfo\u0026gt; textures; }; Notice how for both AddBufferOutput and AddTextureOutput there are three variants: one that defines all the creation parameters (first use of resource), one that defines all of the creation parameters but also adds a clear value/color (again, first use of resource), and one that just takes in a reference to an already created resource. If we look at one of the AddBufferOutput functions, they have three jobs:\nCreate a new TGBBuffer object, which stores all of the info needed to create the resource later. This is stored in TaskGraphBuilder and is shared across all tasks. Create a new TGBBufferTaskInfo object, which is stored directly in the PipelineTaskBuilder. This holds a reference to the actual TGBBuffer resource but also stores extra information relevant to this specific task: does it need to be cleared, whether the resource is being read from or written to, and how it\u0026rsquo;s used in this task (only used for giving better pipeline stage barriers). Update the buffer\u0026rsquo;s lifetime. This is because I am doing memory aliasing, so I need to know what the first and last task each resource is used in. As you can see below, PipelineTaskBuilder covers basically all of the functionality that the compute task builder needs. The only missing piece is that the user also needs to supply the function to be called when the task is being executed every frame. I\u0026rsquo;ll explain this more later, but it\u0026rsquo;s basically just the function that fills out the command buffer and calls the final dispatch call.\nclass ComputeTaskBuilder : public PipelineTaskBuilder { ComputeFunction function; public: ComputeTaskBuilder( TaskGraphBuilder* inBuilder, u16 taskIndex, string_view inName ); void SetFunction( ComputeFunction func ) { function = func; } }; Graphics Tasks The GraphicsTaskBuilder is extremely similar to the ComputeTaskBuilder, so I won\u0026rsquo;t show the code here. It simply adds the ability to add attachments with AddColorAttachment and AddDepthAttachment functions. They follow the exact same pattern as before with three variants: one for creating a new attachment, one for creating and clearing a new attachment, and one for reusing an attachment. It also takes in a function to be called every frame while the task is being executed that will fill out the command buffer and make all the drawIndexed/whatever calls.\nTransfer Tasks Transfer tasks are simply copies and blits on the GPU. So the builder task simply keeps a list of the copies and blits specified. This task does not take in any user function like the compute and graphics tasks, because we can already write the code ahead of time to perform these copies.\nclass TransferTaskBuilder : public TaskBuilder { public: TransferTaskBuilder( TaskGraphBuilder* inBuilder, u16 taskIndex, string_view inName ); void BlitTexture( TGBTextureRef dst, TGBTextureRef src, TextureFilter filter = TextureFilter::LINEAR ); void CopyBuffer( TGBBufferRef dstBuff, u64 dstOffset, TGBBufferRef srcBuff, u64 srcOffset, u64 size ); vector\u0026lt;TGBTextureTransfer\u0026gt; textureBlits; vector\u0026lt;TGBBufferTransfer\u0026gt; bufferCopies; }; The transfer task is something I\u0026rsquo;m sure I\u0026rsquo;ll have to expand later, for things like specifying texture subregions, mips, etc. It also would be really nice to have some easy way to do GPU to CPU transfers for debugging, which could maybe be done nicely with these transfer tasks.\nPresent Tasks These are very simple. The only member function they have is SetPresentationImage, which just takes in a TGBTextureRef to the texture you want to present to the screen. There is no function to run each frame; the only thing this task does is transition the image\u0026rsquo;s layout to VK_IMAGE_LAYOUT_PRESENT_SRC_KHR.\nExternal (Non-Transient) Resources One important point here is that any resource declared using the methods above is transient. This means they are expected to only have lifetimes less than or equal to one full frame. This also means that any resources that need to be persistent, like TAA history buffers, need to be created externally. You also sometimes have resources that are created before the task graph is created, like the swapchain. While these allocations might be handled externally, we still want those resources to be valid inputs, outputs, and attachments for any task in the graph. The way I handle this is by letting the user register external resources. One example would be:\nTGBTextureRef swapImg = builder.RegisterExternalTexture( \u0026#34;swapchainImg\u0026#34;, swapchain.GetFormat(), swapchain.GetWidth(), swapchain.GetHeight(), 1, 1, 1, []() { return rg.swapchain.GetTexture(); } ); ... ComputeTaskBuilder* cTask = builder.AddComputeTask( \u0026#34;post_process\u0026#34; ); cTask-\u0026gt;AddTextureInput( litOutput ); cTask-\u0026gt;AddTextureOutput( swapImg ); cTask-\u0026gt;SetFunction( PostProcessFunc ); So, it looks the same as specifying a regular texture inside of a task, except we now also supply a function pointer that returns the desired resource when called. Why a function, and not just supply the resource handle immediately? Because sometimes, like with swapchain images, the resource changes every frame. So, every time the task graph is executed, the first thing it does is call all resource external functions to update the task graph\u0026rsquo;s internal copy of those resources.\nCompilation Now that we have the tasks and resources defined in the TaskGraphBuilder, it\u0026rsquo;s time to create the final TaskGraph object by compiling the task graph builder. By that, I mean create all of the GPU resource handles, do the memory aliasing, create the synchronization barriers, and create the final, non-builder, task objects that will be used in every frame. All of the code for this is in r_taskGraph.cpp, look for TaskGraph::Compile.\nResource Creation The first step is to create all of the textures and buffers used by the task graph, which is done in TaskGraph::Compile_BuildResources. This simply loops over all the TGBTexture and TGBBuffer resources in the builder and creates the corresponding renderer objects Gfx::Texture and Gfx::Buffer that have the actual GPU handles. The resource handles are created with vkCreate[Image/Buffer], but the resources aren\u0026rsquo;t bound to any device memory just yet; that is done later during memory aliasing.\nThe memory aliasing does make this a little more complicated than it normally would be. The code here fills out all of the Texture and Buffer parameters directly instead of using the standard Device::New[Texture/Buffer] functions because those allocate and bind the device memory immediately. We also need to create a ResourceData for each aliasable resource, which is a reference to the resource and its lifetime.\nFor external resources, it\u0026rsquo;s slightly different. Their allocation happens externally, so they skip calling vkCreate[Image/Buffer] and skip creating a ResourceData. They do, however, still fill out all the info they can in the Gfx::Texture and Gfx::Buffer objects, and are stored in the same list as the transient resources. This is so that there is a unified API later during task graph execution:\nBuffer\u0026amp; GetBuffer( TGResourceHandle handle ); Texture\u0026amp; GetTexture( TGResourceHandle handle ); During task execution each frame, they can use these functions to get the handles to their input and output resources, regardless of whether the resource is external or internal. As mentioned a few sections earlier, these slots that hold external resources get updated each frame, which also allows us to have this unified API.\nMemory Aliasing I\u0026rsquo;ll go through how I do aliasing but forewarning that it\u0026rsquo;s a bit messy. I wanted to give it a shot without looking anything up, so it\u0026rsquo;s not the best. It does work, however, and the code can be found r_tg_resource_packing.hpp and r_tg_resource_packing.cpp. You can skip this section if you\u0026rsquo;re not interested in aliasing.\nGeneral Idea You can view memory aliasing as a 2D rectangle packing problem. The X axis is the task index, and it goes from 0 to the final task\u0026rsquo;s index. The Y axis is the amount of memory. So, each ResourceData we created earlier actually defines a rectangle in this coordinate space. For example, if we had a 4MB texture that was used in tasks [0, 1] inclusively, then that defines a rectangle of height 4MB and width 2, starting from task 0. That means we need to allocate at least 4MB of memory, which we will call a MemoryBucket. We then place our 4MB texture rectangle inside of this bucket. If we stop here, that would mean regular, non-aliased resources. We can, however, pack more rectangles inside of the bucket, so long as they do not overlap each other at all. For example, if we had two buffers of size 2MB that were used in tasks [2, 3] and [6, 7] respectively, we can pack them vertically in our bucket from Y=[0, 2MB] and Y=[2, 4MB]. This is depicted below (excuse my MS Paint graphics):\nExample result of packing 4 resources, labeled by order they were packed Structure This is where it gets a little messy. At the highest level, we pack resources into MemoryBuckets. These buckets keep track of occupied regions on the X axis, which I call TimeSlices. Each TimeSlice then keeps track of the used regions along the Y axis as a list of Blocks, which are just offset and size pairs. If it\u0026rsquo;s confusing, the TLDR is that we just need some system to keep track of where the free rectangles are in our bucket.\nAlgorithm I\u0026rsquo;m sure there are better methods, but the way I did it is the greedy O(N^2) approach:\nSort the ResourceData objects from largest to smallest. Loop through all of the ResourceData: Has this resource been placed into a bucket yet? If yes, then skip until we get an unplaced resource. Create a bucket with the same size as the current resource and place the resource inside of it. This means our bucket has one TimeSlice so far, and that slice has no free blocks in it (because it\u0026rsquo;s the same size as the bucket). Now loop over all remaining resources and try to fit them into the current bucket (see MemoryBucket::AddResource). This is the hardest part, bookkeeping-wise. It works by: Keep track of which blocks on the Y axis are free so far in a list. At the beginning, we assume the entire region from [0, bucketSize] is free, so there is only one element in the list. Loop over all of the time slices in the bucket and find which ones overlap with our current resource (on the X axis). A single resource could overlap many time slices. For each overlapping time slice, update our list of free blocks to remove any regions that would overlap with the Blocks in these time slices. If there are any free regions left that are at least the size of our resource, we can fit the resource in our bucket. There can be multiple offsets (Blocks) that would fit the resource. In that case, I always choose the one with the lowest offset. Go back through the overlapping time slices and update their free regions to account for adding the current resource. Add new time slices for regions that previously had no resources in them. The last couple steps can be slightly confusing, so think of the image example above. After the first three resource are packed, we need to add the fourth (orange) resource. There will be three time slices at this point. The first one doesn\u0026rsquo;t get touched, but the second and third ones need to be updated to show that the block from [2, 3MB] is no longer free. We also need to add a new time slice from [4,5], with openings from [0, 2MB] and [3, 4MB], since no time slice existed there before.\nI liked doing it this way because while the bookkeeping was complicated to track all of the free regions, the idea is pretty straightforward. It also means we\u0026rsquo;re allocating memory for as few large resources as we can, hopefully saving a lot of space.\nFinal Steps The final step is to actually allocate GPU memory and bind resources to it. I use VMA for memory management, which makes this part a lot easier. I highly recommend it. What we do here is loop over each MemoryBucket and allocate one chunk of memory to it using vmaAllocateMemory. Then, we loop over each resource in that bucket and either call vmaBindImageMemory2 or vmaBindBufferMemory2 depending on the resource type, to actually bind the resource to that chunk of memory. These functions conveniently take in the resource Y offsets that we calculated during packing to keep everything non-overlapping. The final thing we do is allocate all of the VkImageView\u0026rsquo;s, since Vulkan doesn\u0026rsquo;t let you do that for a VkImage until it has some memory backing it.\nOne small detail I didn\u0026rsquo;t mention earlier is that some resources have alignment and memory type requirements. So during the packing process, when you\u0026rsquo;re calculating if 2 blocks overlap, you have to take their aligned offsets and sizes into account. You also have to check if the bitwise AND between their VkMemoryRequirements::memoryTypeBits and if they result in 0, you cannot pack them together at all.\nSynchronization and Tasks Now that all of the resources are created and allocated, the final thing to do is to create the tasks and any barriers necessary. In my opinion, this is where task graphs really shine. Since all of the resources and their usages are defined up front, we can deduce the necessary barriers automatically by tracking how each resource is used, from task to task. This tracking is done by creating a list of objects to store a resource\u0026rsquo;s\nstruct ResourceTrackingInfo { ImageLayout currLayout = ImageLayout::UNDEFINED; u16 prevTask = NO_TASK; TaskType prevTaskType = TaskType::NONE; ResourceState prevState; // read, read_write, write ResourceType prevResType = ResourceType::NONE; // buffer, texture, color attach, etc }; I don\u0026rsquo;t do anything fancy here, and really just walk through each case for each task. The code is pretty verbose (see r_taskGraph.cpp::Compile_SynchronizationAndTasks), but I\u0026rsquo;ll show a couple small examples from the Compile_ComputeTask code path for clarity:\nfor ( const TGBTextureTaskInfo\u0026amp; texInfo : bpTask-\u0026gt;textures ) { TGResourceHandle index = texInfo.ref.index; const TGBTexture\u0026amp; buildTexture = builder.textures[index]; ResourceTrackingInfo\u0026amp; trackInfo = builder.texTracking[index]; ResourceType resType = ResourceType::TEXTURE; ImageLayout desiredLayout = InferImageLayout( taskType, texInfo.state, buildTexture.format ); ... if ( trackInfo.prevTask != NO_TASK \u0026amp;\u0026amp; trackInfo.prevState != ResourceState::READ ) { // barrier to wait for any previous writes to be complete VkImageMemoryBarrier2 barrier = GetImageBarrier( index, trackInfo.prevTaskType, trackInfo.prevState, trackInfo.prevResType, taskType, texInfo.state, resType, trackInfo.currLayout, desiredLayout ); pTask-\u0026gt;imageBarriers.push_back( barrier ); } else if ( trackInfo.prevTask == NO_TASK ) { // no writes to wait for, but a barrier is still needed to transition the image to the correct layout TG_ASSERT( trackInfo.currLayout == ImageLayout::UNDEFINED, \u0026#34;Should be the first use of this texture\u0026#34; ); VkImageMemoryBarrier2 barrier = GetImageBarrier( index, TaskType::NONE, ResourceState::UNUSED, trackInfo.prevResType, taskType, texInfo.state, resType, trackInfo.currLayout, desiredLayout ); pTask-\u0026gt;imageBarriers.push_back( barrier ); } ... For full details, please see the code. The main thing here in all of these functions is checking to see if the resource is switching to reads or writes, then writing code (if needed) for all of the cases: R-\u0026gt;R, R-\u0026gt;W, W-\u0026gt;W, W-\u0026gt;R. I make the assumption that back to back writes need to have a barrier inbetween them, though this would be suboptimal if the shaders actually wrote to different regions of the resource. If the resource is a render target attachment however,\nHandling Clears Each task holds a list of buffer and image barriers to execute before running the actual core function of the task. The compute and graphics tasks also hold lists of any resource clears that need to be performed before the core function is run. Clears can only happen on the first use of the resource, so we don\u0026rsquo;t need barriers before the clear happens. Well, that is true for buffers, but sadly not for images. To do a generic vkCmdClearColorImage on an image, we need make sure it has ImageLayout::TRANSFER_DST. So have one extra list of barriers for images, that can run before clear operations. This only applies to non-attachment images for the record. Attachments that want to be cleared use VK_ATTACHMENT_LOAD_OP_CLEAR instead of vkCmdClearColorImage. We also still need barriers after the clear happens, to wait for it to be done before reading or writing to those resources.\nRender Passes For graphics tasks, this is also the step that needs to create the render pass, or gather the information needed for the VkRenderingAttachmentInfo if you are using dynamic rendering. This is very straight forward, since we have a list of all the attachments already, and the storeOp and loadOp can easily be deduced. I actually wrote the first iteration of this task graph using render pass objects, and later switched to dynamic rendering. I found that it was basically the same amount of bookkeeping, so there is little impact either way you choose.\nFinal Notes There are a couple final things to note here. Ideally we would think about batching barriers together and reordering tasks to minimize the number of barriers and batches. That would give better performance, but again is one of the things I intentionally ignored for this engine. To be honest, it\u0026rsquo;s also unclear to me how to reorder tasks to minimize both barriers and maximize memory aliasing at the same time. The second thing to note here, is that the barriers need the VkImage/VkBuffer handles. Since we have external resources that don\u0026rsquo;t supply that handle until execution time, the resource index is instead stored in the barrier handle:\nVkImageMemoryBarrier2 barrier{ VK_STRUCTURE_TYPE_IMAGE_MEMORY_BARRIER_2 }; barrier.image = reinterpret_cast\u0026lt;VkImage\u0026gt;( imageIndex ); Then during execution time, we use that index to lookup the now-valid resource handle and set it appropriately.\nExecution At this point, the task graph is ready to be used during each frame\u0026rsquo;s rendering. The steps are pretty straight forward:\nCall every external resource\u0026rsquo;s function to update the task graph\u0026rsquo;s copy of those resource handles Loop over each task in order and do the following: Submit any pre-clear image barriers, if the task has any Perform any buffer/image clears, if the task has any Submit the regular (not pre-clear) buffer and image barriers Call the user function in the case of compute and graphics tasks, or perform the transfers for transfer tasks The last piece to talk about, is how user task functions look. Here is an example from that frustum culling task shown at the beginning of the article:\nvoid ComputeFrustumCullMeshes( ComputeTask* task, TGExecuteData* data ) { ... CommandBuffer\u0026amp; cmdBuf = *data-\u0026gt;cmdBuf; cmdBuf.BindPipeline( PipelineManager::GetPipeline( \u0026#34;frustum_cull_meshes\u0026#34; ) ); cmdBuf.BindGlobalDescriptors(); struct ComputePushConstants { VkDeviceAddress cullDataBuffer; VkDeviceAddress outputCountBuffer; VkDeviceAddress indirectCmdOutputBuffer; u32 numMeshes; u32 _pad; }; ComputePushConstants push; push.cullDataBuffer = data-\u0026gt;frameData-\u0026gt;meshCullData.GetDeviceAddress(); push.outputCountBuffer = task-\u0026gt;GetOutputBuffer( 0 ).GetDeviceAddress(); push.indirectCmdOutputBuffer = task-\u0026gt;GetOutputBuffer( 1 ).GetDeviceAddress(); push.numMeshes = meshNum; cmdBuf.PushConstants( push ); cmdBuf.Dispatch_AutoSized( meshNum, 1, 1 ); } You can see that the function is able to get the task-specific resources directly, like task-\u0026gt;GetOutputBuffer( 0 ). The number passed in is the order those resources were defined in the builder task, so the order of AddBufferOutput matters. You could do this differently to be less error-prone, but I thought a direct array lookup was nice and I haven\u0026rsquo;t had any issues with it so far.\nConclusion Hopefully that whirlwind tour made sense; there was a lot of code to show, describe, or skip entirely. There are a lot of ways this task graph system isn\u0026rsquo;t ideal, and plenty of ways I would love to extend it more in the future. Even as it is right now though, I find it tremendously helpful. I hope it serves as an example for people getting started on smaller project task graphs, because you can write them without too much hastle in the end, and you should because they\u0026rsquo;re so helpful with modern APIs.\n","permalink":"https://liamtyler.github.io/posts/task_graph/","summary":"Render Graphs, or \u0026ldquo;Task Graphs,\u0026rdquo; rapidly became the standard in the industry ever since Yuriy O\u0026rsquo;Donnell\u0026rsquo;s 2017 GDC presentation on Frostbite\u0026rsquo;s FrameGraph. They provide many benefits, like handling all of the synchronization barriers and resource transitions, reordering tasks for increased performance, reducing memory usage via resource aliasing, and the list goes on for miles. They’re nearly mandatory for large projects these days, given how verbose and error-prone these things would be if done manually with modern APIs like Vulkan and DX12.","title":"A Poor Man's Render Graph"},{"content":"Asset pipelines are one of the pieces of game development I don\u0026rsquo;t see people talk about very often. The main concepts are often introduced and chatted about theoretically, sure: \u0026ldquo;convert your assets into binary packages, compress them, and load them at runtime,\u0026rdquo; but rarely do I find people chatting about the implementation details. It always strikes me as a little odd, because I\u0026rsquo;d argue it has the single biggest impact on development. How fast can the game load? How long do you have to wait to see the results of a change you make? How quickly can people add new assets? How quickly can developers add new asset types? All of these have a huge impact on both artists and programmers alike.\nBecause of this, in this post, I want to walk through how I handle assets in my little hobby engine called Progression. It doesn\u0026rsquo;t introduce any novel ideas, but I hope by sharing it, people can learn a bit and get more ideas of what to do (and what not to do) for their own asset pipelines. In this part 1 post, I will cover the overall asset pipeline, focusing on how the Converter is structured. In part 2 I will go over more specifics of how individual assets are converted (like how textures are composited and compressed).\nContents High-Level Goals Before Asset Conversion Asset Files Parsing .paf Files Scanning The Scene For Used Assets Referenced Assets Non-Inferable Assets Asset Conversion What Exactly Is A Converted Asset Is The Asset Is Out-Of-Date Asset Versioning Finally Converting The Asset Common Pitfall Parallelization Creating The Fastfiles Inter-Asset Dependencies + Asset Ordering Versioning Getting Assets In Engine A Few Final Remarks High-Level Goals It\u0026rsquo;s important to realize what you want most out of your asset pipeline. The entire reason I made one in the first place, is because slow iteration times make my brain short-circuit. There\u0026rsquo;s nothing more frustrating than making a tiny change and then needing to wait several seconds (or often minutes at real studios) to see if it worked. That\u0026rsquo;s why for me, the number one goal was to have fast load times. As for the other aspects:\nConvert Times: Medium priority for me, since it directly impacts iteration speed but happens less often (for me) than booting the game. If you\u0026rsquo;re in a studio with many people converting? I\u0026rsquo;d argue it should be extremely high priority, though it often seems to get pushed to the side. Runtime Performance and Quality: High priority. How much you do offline directly impacts how fast the engine can process and render things. I usually make the choice to favor high FPS, but if you find yourself annoyed at long convert times, then it can definitely be beneficial to sacrifice a little FPS for faster iteration times, especially in development builds. Disk Size: Low priority, since I\u0026rsquo;m just making small demos and not shipping a game. I\u0026rsquo;m actually really passionate about this for real studios (the Call of Duty download sizes make me want to cry) and I think compression is super cool. But for a hobby engine? It hardly matters until you\u0026rsquo;re about to ship. Ease of Adding New Assets and Asset Types: Low priority, since I just do small scenes, and rarely need to add new asset types after the initial set (textures, models, pipelines, scripts, etc). Useability: Medium-high priority. I wanted a simple and consistent interface to access assets, both in the engine C++ code, and in game scripts. Before Asset Conversion So how does this all work in my engine? Well, there are 3 main executables:\nEngine.exe: the game Converter.exe: runs before the game. Responsible for processing all of the assets that a scene will need when loaded in the Engine. It loads them, converts them all to binary, and groups them into binary packages. I call those packages \u0026ldquo;Fastfiles\u0026rdquo;, because that was what they were called in CoD, and I got used to it. ModelExporter.exe: responsible for taking source model files (.obj, .fbx, .gltf, etc) and converting them into a common model file format (.pmodel) that the Converter can then use. This could have all been part of the Converter, but parsing model files can be really involved and sometimes the output can need some cleanup before being used in your real pipeline. So, it was helpful to have as a separate executable that runs once on every model you download, and then never again. We\u0026rsquo;ll walk through from the beginning of the pipeline when the Converter starts up, to loading a fastfile in the Engine. The ModelExporter won\u0026rsquo;t be discussed further here.\nAsset Files In order to use an asset in Progression, it first has to be described in a Progression Asset File (.paf). These are just JSON text files which contain all of the info needed to load an asset. I don\u0026rsquo;t really recommend JSON in hindsight, but it is convenient to use something like json or markup languages that already have fast parsers written for them. Here is an example of a couple assets:\n[{ \u0026#34;Image\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;kloppenheim\u0026#34;, \u0026#34;equirectangularFilename\u0026#34;: \u0026#34;images/skies/kloppenheim_06_2k.exr\u0026#34;, \u0026#34;semantic\u0026#34;: \u0026#34;ENVIRONMENT_MAP\u0026#34; }}, { \u0026#34;Pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;frustum_cull_meshes\u0026#34;, \u0026#34;computeShader\u0026#34;: \u0026#34;frustum_cull_meshes.comp\u0026#34; }}] You can see that each asset first defines the AssetType (Image and Pipeline above). Every asset also needs to have a name. In my engine, the name is the GUID. Different asset types can share the same name, but not two assets within the same type. I know some people use hashes for GUIDs, but I find using plain text names extremely convenient for readability, searchability, and debugging. I highly recommend them. Beyond the name, the parameters are specific to the asset and are used to define how to load the asset. These JSON definitions are directly parsed into [AssetType]CreateInfo structures which look like this:\nstruct BaseAssetCreateInfo { string name; }; struct ModelCreateInfo : public BaseAssetCreateInfo { string filename; // relative path to pmodel file bool recalculateNormals = false; }; ... So the first thing the Converter does, is parse every single .paf file into a bunch of [AssetType]CreateInfo structs. This defines the list of assets that a scene can reference, and all the info needed to load them, if so.\nParsing .paf Files How exactly you go from JSON -\u0026gt; CreateInfo isn\u0026rsquo;t super important, so I\u0026rsquo;ll just cover the main points here. The full code for this can be seen here in asset_parer.hpp, asset_parser.cpp, and asset_file_database.cpp.\nJust like how each asset type defined an [AssetType]CreateInfo struct that derived from BaseAssetCreateInfo, each type also defines an [AssetType]Parser that derives from BaseAssetParser. This is just so we can hold the parsers in a single array BaseAssetParser* g_assetParsers[ASSET_TYPE_COUNT], and then call a virtual Parse function that parses the JSON and returns a filled out [AssetType]CreateInfo. The caveat is that there is one extra level of inheritance with a templated class, to actually allocate the specific CreateInfo type.\nOne interesting aspect to consider here is inheritance (or parenting) in the JSON itself. Say you spent a lot of time filling out a complicated asset with many parameters. Later on it turns out you need to make 3 more just like the first one, with only 1-2 parameters changed. You could copy and paste the original asset definition 3 times, or you could declare that the 3 new assets inherit from that original asset. That way every parameter is copied except the name, and you only have to specify new parameters instead of all of them. In Progression this is done by specifying a \u0026ldquo;parent\u0026rdquo; parameter in the JSON. Then in the code, it is used like this:\nstd::shared_ptr\u0026lt;BaseAssetCreateInfo\u0026gt; parentCreateInfo = nullptr; if ( value.HasMember( \u0026#34;parent\u0026#34; ) ) parentCreateInfo = FindAssetInfo( assetType, value[\u0026#34;parent\u0026#34;].GetString() ); std::shared_ptr\u0026lt;BaseAssetCreateInfo\u0026gt; info; info = g_assetParsers[assetType]-\u0026gt;Parse( value, parentCreateInfo ); ... ... virtual BaseInfoPtr Parse( const rapidjson::Value\u0026amp; value, ConstBaseInfoPtr parentCreateInfo ) override { // create the specific [AssetType]CreateInfo, not a BaseAssetCreateInfo auto info = std::make_shared\u0026lt;DerivedInfo\u0026gt;(); // if this asset has a parent, copy all of the parameters except the name if ( parentCreateInfo ) *info = *std::static_pointer_cast\u0026lt;const DerivedInfo\u0026gt;( parentCreateInfo ); const std::string assetName = value[\u0026#34;name\u0026#34;].GetString(); info-\u0026gt;name = assetName; // finally, fill out the createInfo data by parsing the JSON return ParseInternal( value, info ) ? info : nullptr; } The caveat here, is that I only do a single-pass over the asset files. So, if an asset wants to use a parent, that parent must be defined earlier in the same asset file. I personally don\u0026rsquo;t mind this restriction, and I even think it helps keep things more contained. I definitely remember artists complaining about this at SHG though, which also used single pass asset parsing.\nScanning The Scene For Used Assets Now that we have the possible assets parsed and ready, the next step is to actually figure out which of those are needed for the scene. In my pipeline, the Converter is run per-scene like so: Converter.exe [sceneName]. My scene files are also JSON:\n[{ \u0026#34;Camera\u0026#34;: { \u0026#34;position\u0026#34;: [ -15, -25, 0 ], \u0026#34;rotation\u0026#34;: [ 0, 0, 0 ], \u0026#34;nearPlane\u0026#34;: 0.02 }}, { \u0026#34;Skybox\u0026#34;: \u0026#34;kloppenheim\u0026#34; }, { \u0026#34;Script\u0026#34;: \u0026#34;cameraController\u0026#34; }, { \u0026#34;DirectionalLight\u0026#34;: { \u0026#34;color\u0026#34;: [ 1, 1, 1 ], \u0026#34;direction\u0026#34;: [ 0, 0, -1 ] } }, { \u0026#34;Entity\u0026#34;: { \u0026#34;NameComponent\u0026#34;: \u0026#34;dragon\u0026#34;, \u0026#34;Transform\u0026#34;: { \u0026#34;position\u0026#34;: [ 3, 0, 0 ], \u0026#34;rotation\u0026#34;: [ 90, 0, 90 ], \u0026#34;scale\u0026#34;: [ 1, 1, 1 ] }, \u0026#34;ModelRenderer\u0026#34;: { \u0026#34;model\u0026#34;: \u0026#34;dragon\u0026#34;, \u0026#34;material\u0026#34;: \u0026#34;blue\u0026#34; } }}] So in the example scene above, the converter would need to convert: the image \u0026lsquo;kloppenheim, the script \u0026lsquo;cameraController\u0026rsquo;, the model \u0026lsquo;dragon\u0026rsquo;, and the material \u0026lsquo;blue\u0026rsquo;.\nReferenced Assets Assets can also implicitly reference other ones, so once the scene is parsed we call AddReferencedAssets on each of these assets. For example:\nvoid GfxImageConverter::AddReferencedAssetsInternal( ConstDerivedInfoPtr\u0026amp; imageInfo ) { if ( imageInfo-\u0026gt;semantic == GfxImageSemantic::ENVIRONMENT_MAP ) { auto irradianceInfo = std::make_shared\u0026lt;GfxImageCreateInfo\u0026gt;( *imageInfo ); irradianceInfo-\u0026gt;name = imageInfo-\u0026gt;name + \u0026#34;_irradiance\u0026#34;; irradianceInfo-\u0026gt;semantic = GfxImageSemantic::ENVIRONMENT_MAP_IRRADIANCE; AddUsedAsset( ASSET_TYPE_GFX_IMAGE, irradianceInfo ); auto reflectionProbeInfo = std::make_shared\u0026lt;GfxImageCreateInfo\u0026gt;( *imageInfo ); reflectionProbeInfo-\u0026gt;name = imageInfo-\u0026gt;name + \u0026#34;_reflectionProbe\u0026#34;; reflectionProbeInfo-\u0026gt;semantic = GfxImageSemantic::ENVIRONMENT_MAP_REFLECTION_PROBE; AddUsedAsset( ASSET_TYPE_GFX_IMAGE, reflectionProbeInfo ); } } You can see for images that are environment maps, we also generate two additional images: the irradiance map, and the reflection probe. These are later used for IBL lighting by the renderer. This can be used by any asset type. Pipeline assets add the individual shaders that they reference, for example.\nNon-Inferable Assets While parsing the scene this way gets most of the assets you need, what about assets that your Lua scripts might try to load? The only way to handle these is to explicitly make a list of assets that might be used by the script. For Progression, scene files are stored as [sceneName].json. But the Converter also checks to see if there exists a corresponding [sceneName].csv file in the same directory when processing a scene. If it does, then it loads this file in addition to the .json one. These files are simply lists of assets, in the form [AssetType],[AssetName] on each line.\nOne other area of non-inferable assets is scene-agnostic assets that are not tied to any game objects. One example of this would be compute shaders. These are also handled by adding them to .csv files like assets in scripts, but since these .csvs are agnostic to real scenes, they are stored in a special directory: assets/scenes/required/. When the Converter runs it processes every file in this directory, to always make sure the required assets are up to date and available. The Engine then usually manually loads these fastfiles at startup, like how the renderer loads the gfx_required fastfile to get all of the shaders it needs.\nAsset Conversion Now that we have the list of assets the scene uses, and all of their CreateInfo\u0026rsquo;s, it\u0026rsquo;s time to actually convert them. If you want to see the full code, look at ConvertAssets() in converter_main.cpp, all of base_asset_converter.hpp, and at each asset type\u0026rsquo;s converter. Just like the BaseAssetCreateInfo and BaseAssetParser pattern, there is also a BaseAssetConverter class:\nusing ConstBaseCreateInfoPtr = const std::shared_ptr\u0026lt;const BaseAssetCreateInfo\u0026gt;; class BaseAssetConverter { public: const AssetType assetType; BaseAssetConverter( AssetType inAssetType ) : assetType( inAssetType ) {} virtual ~BaseAssetConverter() = default; virtual string GetCacheName( ConstBaseCreateInfoPtr\u0026amp; baseInfo ) { return \u0026#34;\u0026#34;; } virtual AssetStatus IsAssetOutOfDate( ConstBaseCreateInfoPtr\u0026amp; baseInfo ) { return AssetStatus::UP_TO_DATE; } virtual bool Convert( ConstBaseCreateInfoPtr\u0026amp; baseInfo ) { return true; } virtual void AddReferencedAssets( ConstBaseCreateInfoPtr\u0026amp; baseInfo ) {} }; And just like the BaseAssetParser, there is also one extra intermediate base class, to handle the type casting to specic [AssetType]CreateInfo and any type-agnostic conversion code.\ntemplate \u0026lt;typename DerivedAsset, typename DerivedInfo\u0026gt; class BaseAssetConverterTemplate : public BaseAssetConverter { ... What Exactly Is A Converted Asset We haven\u0026rsquo;t actually covered what it means for an asset to be converted yet. In Progression, a converted asset is one that has been loaded, serialized to binary, and saved to a file. Specifically, these files all get saved under the asset cache directory, which is located at [projectDir]/assets/cache/. The filenames all take the pattern [assetName]_[createInfoHash]_[versionNumber].ffi, where .ffi stands for \u0026ldquo;fastfile intermediate\u0026rdquo;. For example, my assets/cache/models/ directory currently looks like this:\ncube_5919176923328749623_v6.ffi dragon_17433001533983433154_v6.ffi sponza_vulkansamples_15064524848871577573_v6.ffi I refer to the [assetName]_[createInfoHash] component as the \u0026lsquo;cache name\u0026rsquo;, and this is what the BaseAssetConverter::GetCacheName function returns. You don\u0026rsquo;t have to use this naming convention exactly, but I am relying on the fact that any changes to the asset\u0026rsquo;s CreateInfo data will change the cache name. A couple of naming alternatives might be:\nJust include the asset\u0026rsquo;s name right into the hash. This would give shorter and more consistent filenames, but I find having the asset name prefix makes for easier debugging when anything goes wrong in the Converter. Don\u0026rsquo;t hash everything, but rather just convert (some or all of) the CreateInfo data directly to a string. For example, if you had an asset that just had a dozen bools, you could just append a 1 or 0 to the cache name for each of the bools: assetName_100011110001_v0.ffi. This is nice because you can fully identify the entire CreateInfo just by looking at the cache name, which makes for more powerful debugging. However, I find most real asset CreateInfos have either a lot of parameters or long string parameters like filenames. As a result, using this style would make the cache name super long and unfeasible. Is The Asset Is Out-Of-Date We only want to actually convert an asset if it\u0026rsquo;s out-of-date. This is the job of the BaseAssetConverter::IsAssetOutOfDate function. There are two components to this:\nHas the asset been converted before? This one is assetType-agnostic and easy: just get the cache name for the asset and check to see if that cache file (.ffi) exists! This is why I said I am \u0026ldquo;relying on the fact that any changes to the asset\u0026rsquo;s CreateInfo data will change what the cache name.\u0026rdquo; It gives us a quick way to see if we\u0026rsquo;ve ever converted a given combination of settings for an asset before, because if it hasn\u0026rsquo;t, then no matching file will be found. If it has been converted before, did any of the asset\u0026rsquo;s source data change? This one depends on the asset type. For example, if we are converting a model asset whose source file is \u0026lsquo;dragon.obj\u0026rsquo;, then we need to check the timestamp on that .obj file. If the timestamp is newer than our cache file\u0026rsquo;s timestamp, then we need to reconvert. For images, you would have to check the source .png file(s) (possibly multiple, for cubemaps) instead. Asset Versioning Assets can change over time. How many parameters they have, their values, how they\u0026rsquo;re serialized, etc. When this happens, it naturally changes what the converted assets binary would be as well. This creates a potential problem: if we update how a model asset is converted, then we need to mark every single model as out-of-date regardless of what the timestamps are. The way this is done is through asset version numbers. Each asset type has a version number, and when you change how an asset is converted, you bump the version number for that asset type. Since these version numbers are included in the cache filename, bumping the version number always causes those assets to be considered out-of-date.\nFinally Converting The Asset Just like all the other classes before this, I use a BaseAsset virtual class, that all the real asset types inherit from:\nclass BaseAsset { public: BaseAsset() = default; virtual ~BaseAsset(); virtual bool Load( const BaseAssetCreateInfo* baseInfo ) { return false; } virtual bool FastfileLoad( Serializer* serializer ) = 0; virtual bool FastfileSave( Serializer* serializer ) const = 0; virtual void Free() {} ... }; A brief explanation of these functions:\nLoad: takes in the CreateInfo and is expected to load the asset from source. Used by the Converter, but compiled out of the Engine, since the Engine should always be loading converted assets, not source assets. FastfileLoad: loads a binary converted asset. Used by the Engine, not the Converter. FastfileSave: serializes a converted asset. Used by the Converter, not the Engine. Free: Engine only, mainly used for freeing up gpu resources the asset might have. So, in the Converter all we really to do with out-of-date assets is call Load with the appropriate CreateInfo, and then FastfileSave:\nvirtual bool ConvertInternal( ConstDerivedInfoPtr\u0026amp; derivedCreateInfo ) { DerivedAsset asset; const std::string cacheName = GetCacheName( derivedCreateInfo ); asset.cacheName = cacheName; if ( !asset.Load( derivedCreateInfo.get() ) ) { LOG_ERR( \u0026#34;Failed to convert asset %s %s\u0026#34;, g_assetNames[assetType], derivedCreateInfo-\u0026gt;name.c_str() ); return false; } if ( !AssetCache::CacheAsset( assetType, cacheName, \u0026amp;asset ) ) { LOG_ERR( \u0026#34;Failed to cache asset %s %s (%s)\u0026#34;, g_assetNames[assetType], derivedCreateInfo-\u0026gt;name.c_str(), asset.cacheName.c_str() ); return false; } return true; } The AssetCache::CacheAsset simply opens the appropriate .ffi file, and then calls FastfileSave to serialize the asset\u0026rsquo;s data into the file.\nCommon Pitfall Sometimes, when you\u0026rsquo;re adding a new asset, you mess up and have some bugs in either Load or FastfileSave. If the bug causes the Converter to crash while saving the .ffi file, this causes an issue. When you fix the bugs and go to run the Converter again, it will see that the .ffi exists, with a brand new timestamp, and think the asset is up to date, even though it was only a partially written file before the crash! The only way to fix the issue at that point, would be to either force convert the asset that failed, or delete the invalid .ffi file.\nThe way I try to mitigate this happening is first by not opening the .ffi until Load has fully finished. Second, I wrap the call to FastfileSave in a try/catch block, and if there is an exception I delete the file in the catch block. This seems to work reasonably well, though I think a better way might just be to write to a temporary .ffi file, and then if everything succeeds, rename that file to the intended cache name. That way, if your computer blue screens or you lose power while an asset is being serialized, you could just run the Converter again like normal.\nParallelization With the system described so far, scenes need to be processed one at a time, loaded single-threaded. For my engine, that\u0026rsquo;s really not an issue, since it\u0026rsquo;s very quick. The actual asset conversion is the important piece to parallelize. Fortunately, we\u0026rsquo;ve set things up so that each asset can be processed independently, all they need is their CreateInfos. So, I just get the list of all the out-of-date assets, and then convert them all in parallel using OMP. This works great, though the one caveat is that by default OMP doesn\u0026rsquo;t allow nested parallel calls. So if any of your asset Load functions are parallelized the same way, you can consider adding omp_set_nested( 1 ); to the start of the Converter to allow nested parallelization. I found this helpful because my environment maps are pretty slow to process, so the converter would stall waiting for that to finish without nested parallelization.\nCreating The Fastfiles At this point in the Converter, every asset has been converted and is up to date, so it\u0026rsquo;s time to create the fastfile. I haven\u0026rsquo;t mentioned what the fastfile actually is yet though: it\u0026rsquo;s simply all of the converted assets bundled into one file. This is to make load times faster by just having a single file IO call that can be read serially, instead of a ton of file IO by using the .ffi files directly. You can add whatever metadata you want, but currently, my fastfiles are literally just lists of [AssetType][AssetBinary] pairs.\nNow the question is: do we need to rebuild the fastfile? Well, it\u0026rsquo;s very similar to checking for out-of-date assets, with one extra case:\nDoes the fastfile exist? I store all of mine in the asset/cache/fastfiles/ with the naming convension [sceneName]_[version].ff. If this file is not found, the fastfile must be built. If the fastfile does exist, then we need to compare the fastfile\u0026rsquo;s timestamp, to every single asset used in the scene. If any of the asset timestamps are newer than the fastfile, it means the fastfile is out-of-date. It\u0026rsquo;s not enough to just check if ( numberOfOutOfDateAssets \u0026gt; 0 ), because the assets could have been converted from a different scene, but are still newer than the current scene\u0026rsquo;s fastfile. Finally, we need to check if the list of assets we would put in the fastfile if we build it, is different than the list of assets that are already in the previously built fastfile. To track this, every time a fastfile is built I export a text file of [AssetType],[AssetName] pairs for every asset used in that fastfile, and store it in asset/cache/assetlists/[sceneName].txt. The next time the converter is run, we can compare the current list of assets to the ones in this textfile. If the lists differ at all, the fastfile must be rebuilt. Inter-Asset Dependencies + Asset Ordering Sometimes assets reference other assets, like a material referencing its albedo and normal textures. So, when we actually deserialize these assets in Engine, we either need to make sure the images were already loaded before the materials get loaded, or do two-pass loading to fixup the references. I chose the first option for simplicity. This means, however, that the order of assets in our fastfile matters. I do this by grouping the assets by type, and having an explicit type order:\nenum AssetType : u8 { ASSET_TYPE_GFX_IMAGE = 0, ASSET_TYPE_MATERIAL = 1, ASSET_TYPE_SCRIPT = 2, ASSET_TYPE_MODEL = 3, etc... }; As you can see, all of the images are first. That\u0026rsquo;s because they don\u0026rsquo;t reference anything else. Materials need to go after images because they reference images. Scripts don\u0026rsquo;t reference anything, so they can be ordered anywhere, but models can reference materials, so they have to be after materials.\nVersioning Just like converted assets, we have to consider when we make changes to the converter. If we change the version number on any of the assets, we have to rebuild the fastfile. We also need to have a separate version number just for fastfiles for when the fastfile serialization or metadata is changed, independent of converted assets. In Progression it looks like this:\nconstexpr i32 g_assetVersions[] = { 9, // ASSET_TYPE_GFX_IMAGE, \u0026#34;New name serialization\u0026#34; 10, // ASSET_TYPE_MATERIAL, \u0026#34;New name serialization\u0026#34; 1, // ASSET_TYPE_SCRIPT, \u0026#34;New name serialization\u0026#34; 6, // ASSET_TYPE_MODEL, \u0026#34;Add meshlet cull data\u0026#34; etc.... }; constexpr u32 PG_FASTFILE_VERSION = 10 + ARRAY_SUM( g_assetVersions ); // reason The comments are intentionally there so that if two people made different changes and had to bump the same version number, there would be a merge conflict. Without the comment, it would auto-merge to only bump the number by one, even though two changes actually happened.\nOne note here, is that I append the version number to the fastfile\u0026rsquo;s name. This works, but anytime it gets bumped the old fastfiles just lay around and take up space. I think it\u0026rsquo;s a better idea to serialize the version number into the fastfile, and then check to see in Engine if the version number matches what it expects. This would keep the cache directory smaller when version bumps happen.\nGetting Assets In Engine In Progression, the AssetManager handles the loading and storing of every asset. See asset_manager.hpp and asset_manager.cpp for the full code. This is where the LoadFastFile function is implemented, and it\u0026rsquo;s very simple: memory map the fastfile, look at the first asset type, allocate it + call its FastfileLoad, and then move on to the next asset and repeat. There is a hash map per AssetType for storing these:\nunordered_map\u0026lt;string, BaseAsset*\u0026gt; g_resourceMaps[ASSET_TYPE_COUNT]; You can see that I\u0026rsquo;m just using plain pointers, but you probably want your loaded assets to be ref-counted. I haven\u0026rsquo;t added this yet, but mostly because I load one demo scene at a time, and it doesn\u0026rsquo;t really matter :) If you ever manage multiple scenes though, when you unload a scene, you will want to free the assets that are not shared by any other scene.\nAs for accessing these loaded assets, I had decided early on that I wanted to have the interface AssetManager::Get\u0026lt;AssetType\u0026gt;( assetName ). This just seemed nice and simple to me, which is one of the goals I had. An example would be Material* mat = AssetManager::Get\u0026lt;Material\u0026gt;( \u0026quot;wood_floor\u0026quot; );. To make this happen, we need a way to convert from actual template AssetType to its ASSET_TYPE enum value, to appropriately index into g_resourceMaps[ASSET_TYPE_COUNT]. The only way I know how to do that in C++, is to use static variables:\nstruct GetAssetTypeIDHelper { static u32 IDCounter; }; template \u0026lt;typename Derived\u0026gt; struct GetAssetTypeID : public GetAssetTypeIDHelper { static u32 ID() { static u32 id = IDCounter++; return id; } }; We can then do things like GetAssetTypeID\u0026lt;Material\u0026gt;::ID() to get an index from a type. The caveat is that we have to initialize these in the same order their type appears in the ASSET_TYPE enum, which I do at the beginning of AssetManager::Init:\nvoid Init() { GetAssetTypeID\u0026lt;GfxImage\u0026gt;::ID(); // ASSET_TYPE_GFX_IMAGE GetAssetTypeID\u0026lt;Material\u0026gt;::ID(); // ASSET_TYPE_MATERIAL GetAssetTypeID\u0026lt;Script\u0026gt;::ID(); // ASSET_TYPE_SCRIPT etc... And with that, we\u0026rsquo;ve covered the entire asset pipeline from start to finish! I hope that gives some more insight into how these pipelines work.\nA Few Final Remarks There are a few final things I\u0026rsquo;d like to discuss about the whole pipeline, particularly in regards to the high-level goals we initially set out to achieve:\nPerformance: The main goal was to have fast load times. Did we succeed? I\u0026rsquo;d say yes! Here are the load times for two scenes:\nCrytek Sponza: 41ms, for a 77MB fastfile containing 77 assets Intel Sponza: 428ms, for a 1.212GB fastfile containing 86 assets (has much larger models and 4k textures compared to Crytek\u0026rsquo;s). I do have an NVME SSD which helps a lot, but I\u0026rsquo;m still quite pleased with the load times. The load times also include creating and uploading the textures + models to the GPU. There was also a secondary goal of decent Converter performance. Now this one heavily depends on what scene you are converting, but overall, my convert times are OK, but not great. For example, a fully fresh convert of Crytek\u0026rsquo;s Sponza takes 2.5 seconds on my machine, while the Intel Sponza (plus a big skybox) takes 36 seconds. However, the second convert once nothing has changed is only 21ms. This is largely because I haven\u0026rsquo;t taken the time to optimize convert times, and for a hobby engine kind of lean towards slow but simple converters compared to a lot of complicated fast paths.\nHot Reloading: Currently, my engine doesn\u0026rsquo;t support hot reloading of assets. It did, a long long time ago, but it was implemented awkwardly, so I ripped it out. It\u0026rsquo;s something I\u0026rsquo;d definitely like to add again one day, but honestly for Progression? It doesn\u0026rsquo;t add a lot of value, when booting the Engine and loading a Scene takes less than one second. The iteration time is already very low :)\nBreakage Frequency and Debugging: Surprisingly, this breaks less often than you\u0026rsquo;d think! It is fairly easy to add bugs, especially with custom serialization and deserialization. However, I find that with the pipeline\u0026rsquo;s simple structure and naming conventions, it rarely takes me a long time to figure out what mistake I made. In my mind, as long as bugs don\u0026rsquo;t happen too often and are quick to fix, then you\u0026rsquo;ve succeeded at something.\nDisk Size + Compression: This is a huge one that I haven\u0026rsquo;t talked about yet. Any real engine will compress their assets/fastfiles/packages, typically in a format that gives very fast decompression rates (LZ4, Oodle, etc). Progression currently doesn\u0026rsquo;t compress anything however. I would like to add it of course, but part of the reason I haven\u0026rsquo;t bothered yet, is because of some small experiments with LZ4. Since I haven\u0026rsquo;t played around with RDO on my textures, using LZ4 on them usually only saves between 0-2%. Even on the rest of the assets, the savings are not amazing. For example, here are the results of using LZ4 on the entire fastfile for the two Sponza scenes I mentioned above:\nCrytek Sponza: 14.1% savings with default compression, and 20.9% savings with LZ4_HC Intel Sponza: 11% savings with default compression, 18% with LZ4_HC Not really substantial enough to make me add it yet, especially since LZ4_HC isn\u0026rsquo;t super fast. I\u0026rsquo;d rather just keep the fastest iteration times for now. I do love compression, however, so I definitely want to revisit this at some point, and try to use RDO and make assets compression-friendly :)\nAnd with that, I think I\u0026rsquo;ve covered everything I wanted to. I\u0026rsquo;ll definitely post a part 2, going over how specific assets are converted, but I hope the general pipeline structure makes sense, and why I chose to design it like that. Thanks to everyone who stuck around to read the whole thing, this definitely got longer than I thought it would. Leave a comment if you\u0026rsquo;d like; I\u0026rsquo;d love to hear feedback, and love to hear other decisions people made when structuring their asset pipelines!\n","permalink":"https://liamtyler.github.io/posts/asset_pipeline_part_1/","summary":"Asset pipelines are one of the pieces of game development I don\u0026rsquo;t see people talk about very often. The main concepts are often introduced and chatted about theoretically, sure: \u0026ldquo;convert your assets into binary packages, compress them, and load them at runtime,\u0026rdquo; but rarely do I find people chatting about the implementation details. It always strikes me as a little odd, because I\u0026rsquo;d argue it has the single biggest impact on development.","title":"Asset Pipelines Part 1"},{"content":"My name is Liam Tyler. I\u0026rsquo;ve been doing graphics for about 7 years. In terms of work, I was previously a senior rendering engineer at Deviation Games working on an unnanounced UE5 game. Before that, I was a rendering engineer at Sledgehammer Games working on the Call of Duty games. I\u0026rsquo;m currently taking a small sabbatical from work to focus on my health and to hike a bunch of mountains in Colorado this summer.\nThis blog is mostly going to be a combination of various graphics or coding things that I\u0026rsquo;ve found interesting, as well as learnings from working on my little hobby engine.\nYou can contact me at Tyler.Liam7@gmail.com.\n","permalink":"https://liamtyler.github.io/about/","summary":"My name is Liam Tyler. I\u0026rsquo;ve been doing graphics for about 7 years. In terms of work, I was previously a senior rendering engineer at Deviation Games working on an unnanounced UE5 game. Before that, I was a rendering engineer at Sledgehammer Games working on the Call of Duty games. I\u0026rsquo;m currently taking a small sabbatical from work to focus on my health and to hike a bunch of mountains in Colorado this summer.","title":"About Me"},{"content":" In this post, I want to explain how to take a normal map and generate its corresponding height map (also called displacement map). There are multiple ways to achieve this, but the one I\u0026rsquo;ll be covering in this post is the method that Danny Chan presented in Advances in Real Time Rendering at Siggraph 2018 (link here, page 16). It\u0026rsquo;s also the same method that the Call of Duty engine still uses today to generate the vast majority of its height maps. All the code I\u0026rsquo;ll cover in this post is implemented here.\nWhy There are several reasons why having this process can be helpful:\nThe simplest one: very often no height map is available. Even if you aren\u0026rsquo;t going to actually displace the surface, height maps are useful for a number of reasons, such as blending, or generating quality AO as described in Danny\u0026rsquo;s presentation linked above (pages 16-22). Even if there is a height map, it\u0026rsquo;s often \u0026lsquo;wrong\u0026rsquo;. What I mean, is that the slopes of the surface implied by the height map often don\u0026rsquo;t match the slopes in the normal map. A height map and normal map are just two different ways of representing the exact same surface, and they are intrinsically linked together. For example, say you\u0026rsquo;re displacing the ground, and it creates a small 45-degree hill. If your normal map returns a slope of 80 degrees, then you\u0026rsquo;ll be lighting the hill as if it should be steeper, and it will just look off:\nThe majority of painted or edited height maps just won\u0026rsquo;t have this guarantee of matching the normals (or vice versa), and therefore be wrong. While the above image could just have either the heights or normals scaled to fix the mismatch, it\u0026rsquo;s also common with painted normal maps where they end up being \u0026lsquo;impossible\u0026rsquo; (there exists no surface that could match the slopes). In general, normal maps that are captured through photogrammetry are the best ones. However, even if you\u0026rsquo;re using photogrammetry assets, the height map provided with them is often cleaned up or scaled to look \u0026lsquo;pretty good\u0026rsquo;, but not actually made to match the normal map very well (I\u0026rsquo;ll show an example of this later). Nearly all engines provide two extra parameters when doing displacement: displacement scale and bias. In most of those, changing the displacement scale causes the geometric and normal slopes get out of sync. This is because usually the displacement height and normals are calculated completely independently from each other in the shader:\ndisplacementHeight = scale * sampledHeightMapValue + bias;\ndisplacedNormal = regularNormalMappedNormal;\nReally though, changing the displacement scale changes the slope of the geometry, so the normal should be scaled as well. You can play around with the displacement bias as much as you want since that just moves the whole surface, but changes to the scale mean the normals need to be scaled as well. The reverse is true as well: scaling the normals means you need to change the height map or displacement scale too. Because of reasons #2 and #3 above, the Call of Duty pipeline actually doesn\u0026rsquo;t allow you to use a hand-authored height map and a normal map at the same time on a material. You can either:\nProvide only a normal map and have the height map auto-generated from the normal map using the method I\u0026rsquo;ll describe in the rest of this article. Provide only a height map and have the normal map auto-generated from the height map. In both cases, it ensures that the height map and normals will always match. A follow-up question is then \u0026ldquo;what about the displacement scale\u0026rdquo;? The scale is no longer tied to the material, but the textures themselves:\nIf using a normal map, a new float parameter is introduced to scale the normals. The normals will be scaled before generating the height map. If using a height map, then you can simply scale the heights before generating the normal map. In both cases again, the maps will be matching. Now that that\u0026rsquo;s all explained, let\u0026rsquo;s dive into how this normal-to-height generation actually works.\nHow it works Core Idea The algorithm itself is actually pretty short. The core idea is that for any pixel, we can estimate its height given the heights and slopes of its neighbors. Take the image below for example, where we want to estimate the height of the center pixel '?'. Since we have the normal map as input, we know the slope (height derivative) in the X and Y directions for each pixel. So one guess for the middle pixel\u0026rsquo;s height would be L + Slope(L).x. Another guess would be D - Slope(D).y (note that we are assuming that normals the normals are +Y down, and +X to the right, but you can flip the signs if your normal maps are different). We can do this for all 4 neighbors, and take the average to get a pretty good guess for the middle\u0026rsquo;s height.\nYou might be confused though, \u0026ldquo;we don\u0026rsquo;t know the neighbor\u0026rsquo;s heights yet\u0026rdquo;? Correct, but it doesn\u0026rsquo;t matter! The cool thing about this algorithm is that if you just make an initial guess for the heights, and do this neighbor guess averaging over and over, it will eventually converge! So if we start with a guess of all heights = 0 and iterate a bunch of times, it converges to the final height map we want. Pretty neat, right? There are some other details and tricks to speed up the convergence though, so let\u0026rsquo;s get into the specifics.\nImplementation So the first part of the algorithm is to convert the normals into slope space. This is just the partial derivatives of the height field with respect to X and Y. Aka, how much the height changes in the X and Y directions: \\[ \\frac{\\partial z}{\\partial x} = \\frac{-n.x}{n.z} \\] \\[ \\frac{\\partial z}{\\partial y} = \\frac{-n.y}{n.z} \\] vec2 DxDyFromNormal( vec3 normal ) { vec2 dxdy = vec2( 0.0f ); if ( normal.z \u0026gt;= 0.001f ) dxdy = vec2( -normal.x / normal.z, -normal.y / normal.z ); return dxdy; } We do this for every pixel in the normal map and divide by the input texture size. This is just to normalize the slopes to be independent of the texture size. Without it, the height map would get taller as you increased the normal map size for example.\nFloatImage2D dxdyImg = FloatImage2D( width, height, 2 ); const vec2 invSize = { 1.0f / width, 1.0f / height }; for ( int i = 0; i \u0026lt; width * height; ++i ) { vec3 normal = normalMap.Get( i ); dxdyImg.Set( i, DxDyFromNormal( normal ) * invSize ); } The last thing to do before we get to the meat of the algorithm is to allocate two more images: the final output height map (last argument), and a scratch height map. The latter is just used internally by the algorithm during iteration, which we will see in a second.\nFloatImage2D scratchHeights = FloatImage2D( width, height, 1 ); FloatImage2D outputHeights = FloatImage2D( width, height, 1 ); BuildDisplacement( dxdyImg, scratchHeights.data.get(), finalHeights.data.get() ); Now the core of BuildDisplacement is the repeated averaging of the height guesses I explained earlier. This iteration method is also called relaxation. You could do this on the current resolution as-is, but it would take a long time to converge (often tens of thousands, or hundreds of thousands of iterations). As the presentation mentions, to speed up the convergence you can instead: recursively downsample the image, iterate on that a smaller number of times (tens, or hundreds of times, not thousands), and then upsample that to use as the starting guess for next round of iterations. Confusing? Here\u0026rsquo;s the code:\nvoid BuildDisplacement( const FloatImage2D\u0026amp; dxdyImg, float* scratchH, float* outputH ) { int width = dxdyImg.width; int height = dxdyImg.height; if ( width == 1 || height == 1 ) { memset( outputH, 0, width * height * sizeof( float ) ); return; } else { int halfW = Max( width / 2, 1 ); int halfH = Max( height / 2, 1 ); FloatImage2D halfDxDyImg = dxdyImg.Resize( halfW, halfH ); float scaleX = width / static_cast\u0026lt;float\u0026gt;( halfW ); float scaleY = height / static_cast\u0026lt;float\u0026gt;( halfH ); vec2 scales = vec2( scaleX, scaleY ); for ( int i = 0; i \u0026lt; halfW * halfH; ++i ) halfDxDyImg.Set( i, scales * halfDxDyImg.Get( i ) ); BuildDisplacement( halfDxDyImg, scratchH, outputH ); // upsample the lower resolution height map, outputH, into scratchH stbir_resize_float_generic( outputH, halfW, halfH, 0, scratchH, width, height, 0, 1, -1, 0, STBIR_EDGE_WRAP, STBIR_FILTER_BOX, STBIR_COLORSPACE_LINEAR, NULL ); } ... neighbor iteration ... Let\u0026rsquo;s walk through an example image of size 32x32. It would first downsample the slope image to 16x16: halfDxDyImg = dxdyImg.Resize( 16, 16 ). The next step is to update the slopes to account for the bigger pixel footprint. Remember we initially divided the slopes by the texture size: DxDyFromNormal( normal ) * invSize. So each slope in that image was describing \u0026ldquo;how much does the height change if we move 1/32nd of a unit\u0026rdquo;. Each texel in the lower resolution image now describes \u0026ldquo;how much does the height change if we move 1/16th of a unit\u0026rdquo;, so we need to double the slopes from the higher resolution image.\nNext, we call BuildDisplacement again until we get down to a 1x1 image. Here we seed the initial guess of all heights to 0. You can use something other than 0, the only thing it changes is where the average height of the final image is (moves the entire surface up/down). It will then upsample to 2x2 and do the neighbor iteration. Then upsample that to 4x4 and do the neighbor iteration again, then 8x8, etc. So how does the neighbor iteration look exactly?\nfloat* cur = scratchH; float* next = outputH; // ensure numIterations is odd, so the final output is stored in \u0026#39;next\u0026#39; aka outputH numIterations += numIterations % 2; for ( uint32_t iter = 0; iter \u0026lt; numIterations; ++iter ) { #pragma omp parallel for for ( int row = 0; row \u0026lt; height; ++row ) { int up = Wrap( row - 1, height ); int down = Wrap( row + 1, height ); for ( int col = 0; col \u0026lt; width; ++col ) { int left = Wrap( col - 1, width ); int right = Wrap( col + 1, width ); float h = 0; h += cur[left + row * width] + 0.5f * dxdyImg.Get( row, left ).x; h += cur[right + row * width] - 0.5f * dxdyImg.Get( row, right ).x; h += cur[col + up * width] + 0.5f * dxdyImg.Get( up, col ).y; h += cur[col + down * width] - 0.5f * dxdyImg.Get( down, col ).y; next[col + row * width] = h / 4; } } std::swap( cur, next ); } A few notes on this:\nThe neighbor averaging can\u0026rsquo;t be done in place (much like a blur filter), which is why it flips between scratchH and outputH each iteration. The code is written to assume that the final iteration will store into outputH, which means we must have an odd number of iterations (store results in outputH on iteration 1, scratchH on iteration 2, outputH on iteration 3, etc.). You can also see that I just made the assumption that all of these input textures wrap. If they can\u0026rsquo;t wrap, then you can try the usual tricks at the edges (clamp/reflect). To be honest, I haven\u0026rsquo;t tried it yet personally. All of the slopes are multiplied by 0.5. Embarrassingly, I have no idea why this is needed! I found out it gives much better results accidentally when I was trying to average the neighbor slopes with the center pixel\u0026rsquo;s slope (the center slopes cancel out anyways, but the 0.5 multiplier significantly improved things). Perhaps I missed a factor of 2 somewhere else, not sure. After that, the only remaining step is to remap the final image to be 0 - 1, for minimal loss when saving it out (if saving the result out as 8 or 16-bit). The scale needed for remapping (maxHeight - minHeight) is the same needed later in the shader for unpacking. The same goes for the bias (minHeight).\nThe Results It\u0026rsquo;s a bit of a challenge to quantify exactly how good the generated height maps are, particularly when we don\u0026rsquo;t have ground truth images to compare them to! The only ground truth images we have are the normal maps. Because of that, I took the generated height maps and generated normal maps from those. That way we can do direct comparisons and gather standard image diff metrics (MSE/PSNR). Now, there is a huge caveat here: both the normal-to-height and the height-to-normal processes introduce error. So by comparing the two normal maps, we are actually measuring the combined error of both processes, not just the first normal-to-height error that we would like. But it\u0026rsquo;s better than nothing, so here we go.\nBelow are the results of 5 test cases (3 photogrammetry, 2 synthetic). The first column is the starting ground truth normal map, the second column is the generated normal map, and the third column is the generated height map. There are many ways to generate normals from a height field, I chose the simplest one for this, just doing the finite difference between the left + right neighbors, and the up + down neighbors (search for GetNormalMapFromHeightMap in the code). All of these height maps were generated using 1024 iterations. To get the PSNR between the two normal maps, I generated the dot product between both images, adjusted so that 0 means both vectors are the same, and 2 would mean they are completely 180 from each other: 1 - Dot( n1, n2 ). I then found the MSE of that dot product image and converted that to PSNR (search for CompareNormalMaps in the code). Here were the results (click images to enhance and flip between them):\nPSNR = 29.7 PSNR = 30.2 PSNR = 31.4 PSNR = 72.9 PSNR = 50.2 Pretty good in my opinion! If you really squint, you can see where it doesn\u0026rsquo;t do so well. The last synthetic image is a nice example of how it doesn\u0026rsquo;t do well at sharp corners. Some of that is just due to the error in the height-to-normal process, but some of it also is from the height generation.\nCompared To Provided Height Map Remember when I said that sometimes even with photogrammetry, the provided height map just doesn\u0026rsquo;t match the normals that well? The 2nd rock image above is a good example of this.\nThis asset is from Poly Haven, which is wonderful and actually provides a blender file that lets us see what the author says the scale should be. In this case, it\u0026rsquo;s 0.20. However, look what happens when we generate the corresponding normal map, using the provided height map + 0.20 scale:\nPSNR = 20.6 It basically kept all of the flatter top surfaces accurate, but heavily increases all the other slopes. It also has a PSNR of 20.6. If we get rid of the 0.20 scale and do a search for the scale that makes for the best PSNR, we find that a scale of 0.065 gives us a PSNR of 23.5:\nPSNR = 23.5 Now we have the opposite problem: the steeper slopes and crevices were preserved better at the cost of flattening everything else. This means that really, there is no scale that will preserve both areas\u0026ndash; the map itself just doesn\u0026rsquo;t match the normals well. If we use the auto-generated height map, however, you can see that it matches a lot better. Not perfectly, but definitely more than the provided height map can:\nPSNR = 30.2 For the record, I\u0026rsquo;m not blaming the author of this material at all. This is a great asset to have, and I bet it still looks pretty good when displaced. But if your goal is to have the geometric and normal slopes match like we have been arguing it should, then the height map isn\u0026rsquo;t quite the best for it. So just a warning that just because a material was captured with photogrammetry doesn\u0026rsquo;t mean it\u0026rsquo;s always perfect.\nHow Long To Iterate? You might notice that I didn\u0026rsquo;t define numIterations anywhere in the code snippets above. I was glossing over that because it\u0026rsquo;s a bit trickier to answer! The longer you run this algorithm, the longer it has to converge. How do you know when it is done converging? There are a couple of ways you could define convergence and measure it. Since our initial guess was a flat surface, however, a pretty good way you can tell if it has converged is when the surface stops getting taller as you increase the number of iterations. Even for images with lots of crevices and details in the middle height ranges, tracking the total height still seems to be a good way of defining total convergence, at least with all of my test cases. Now, you can track the min and max height after each iteration to do this, but:\nThe height changes are very small per iteration and get increasingly smaller with each further iteration. That means you\u0026rsquo;ve really just swapped to a new, equally hard problem: what should the super small threshold be exactly? It\u0026rsquo;s slightly more annoying and slower to track each iteration since we are multi-threading. The option I took instead was to just try a bunch of different iteration counts on a number of images and see how they converged. Below is the results of 7 different normal maps, where the height map was generated at 9 different iteration counts: 32, 64, 128, 256, 512, 1k, 2k, 4k, and 32k. If we consider the scale given by the 32k image as \u0026lsquo;fully converged\u0026rsquo; we can then divide all of the other scales (per image) by that to normalize everything:\nYou can see that there is some slight variation between normal maps, but they all pretty closely follow the same trend. Basically everything above 2k is imperceptible, and 512-1024 iterations are probably more than enough for the vast majority of images. I default to 512 as being \u0026lsquo;good enough\u0026rsquo;, but you can pick whatever iteration count you feel is most appropriate for your application. For many applications, sub-100 iteration counts can be perfectly fine. I will say that if you are using these as displacement maps though, I have noticed iteration counts under 100 can sometimes (not always) be noticeably not done converging, like where sections of geometry that should be straight are actually curved because it didn\u0026rsquo;t converge yet.\nCan We Make The Algorithm Faster? There are a few obvious ways this code could be improved: the parallelization could be better, the downscaling of the dxdy image and upscaling of the height image could for sure be sped up (I was just using the easiest stb option for them), and you could likely do more intelligent ways of calculating numIterations. Implementing those isn\u0026rsquo;t the point of this post though, the question I want to ask here is: can the algorithm itself be tweaked to be faster? Turns out, the answer is yes!\nTake one of the rock textures from above, which has size 1024x1024. If we save out all of the intermediate height maps (what BuildDisplacement returns) and scale them up to 1024x1024 for comparison, we get the following (sizes 8x8 to 1024x1024):\nYou can see that pattern: as the mips get larger, the less impact each set of iterations has. This makes sense\u0026ndash; you would hope that once all of the iterations on the 1x1 -\u0026gt; 128x128 sized mips are done that all of the details sized \u0026gt;= 1/128th of the image would be done converging. So, there should be less work to do as the image gets larger, because the larger iterations have better and better initial guesses and will mostly just be converging the smaller and less noticeable details.\nSo what if we reduced how many iterations just the largest mips got? To do this, let\u0026rsquo;s introduce an iteration multiplier. It will scale the number of iterations of mip 0 by that amount, and then we can double the multiplier for the next mip. This way we have fewer iterations on the larger mips, but the same amount on the smaller ones:\nvoid BuildDisplacement( const FloatImage2D\u0026amp; dxdyImg, float* scratchH, float* outputH, uint32_t numIterations, float iterationMultiplier ) {\t... BuildDisplacement( halfDxDyImg, scratchH, outputH, numIterations, 2 * iterationMultiplier ); ... numIterations = (uint32_t)(Min( 1.0f, iterationMultiplier ) * numIterations); numIterations += numIterations % 2; // ensure odd number ... The results? If we do 1024 iterations with a 0.25 multiplier on the rocky image above, we get nearly the exact same quality as a 512 iteration with a 1.0 multiplier, but 25% faster. What if the image was larger though? We are predicting that those details matter less and less. It turns out the prediction is true! Doing 1024 iterations with a 0.25 multiplier on a 2048 version of this same image gives the same quality as doing 710 iterations, but again faster than 512. If we do the same with a 4096 version of the image, it gets even better: the same quality as doing 794 iterations, but the same cost as doing 350 iterations!\nSo what multiplier should you use? Like the number of iterations, it just depends on what quality you want and how fast you want it. Remember this also works better on larger images like 2k or 4k. I like to default to 1024 iterations with 0.25 multiplier, especially with 1k+ images, but adjust it as you see fit!\nFuture Work While this method works well (don\u0026rsquo;t forget it\u0026rsquo;s used in AAA games like Call of Duty!), there are a lot of questions and issues we ignored:\nSince the height algorithm always pulls from its neighbors, it\u0026rsquo;s guaranteed to mess up (smooth) sharp corners Similarly, the way we reconstructed the normals increases how much error there is at sharp corners, making it harder to examine the error from each step Explaining why as you increase the iteration count, the PSNR almost always goes down slightly If you zoom in and compare the original and generated normal maps, you\u0026rsquo;ll notice the generated one has most of the high frequency details blurred out Are there alternative ways to generate the height maps that might be better or faster? And more. I think this post is definitely long enough as-is, however, so I\u0026rsquo;m going to write a part 2 where I dig deeper into these issues and try to answer some of these questions. Once that is posted, I\u0026rsquo;ll update this page with a link to it.\nCredits Huge thanks to the providers of the source normal maps (list of each one, and links to them can be found here.\n","permalink":"https://liamtyler.github.io/posts/normal-to-height/","summary":"In this post, I want to explain how to take a normal map and generate its corresponding height map (also called displacement map). There are multiple ways to achieve this, but the one I\u0026rsquo;ll be covering in this post is the method that Danny Chan presented in Advances in Real Time Rendering at Siggraph 2018 (link here, page 16). It\u0026rsquo;s also the same method that the Call of Duty engine still uses today to generate the vast majority of its height maps.","title":"Generating Height Maps From Normal Maps"}]