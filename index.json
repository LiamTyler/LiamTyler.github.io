[{"content":"Asset pipelines are one of the pieces of game development I don\u0026rsquo;t see people talk about very often. The main concepts are often introduced and chatted about theoretically, sure: \u0026ldquo;convert your assets into binary packages, compress them, and load them at runtime,\u0026rdquo; but rarely do I find people chatting about the implementation details. It always strikes me as a little odd, because I\u0026rsquo;d argue it has the single biggest impact on development. How fast can the game load? How long do you have to wait to see the results of a change you make? How quickly can people add new assets? How quickly can developers add new asset types? All of these have a huge impact on both artists and programmers alike.\nBecause of this, in this post, I want to walk through how I handle assets in my little hobby engine called Progression. It doesn\u0026rsquo;t introduce any novel ideas, but I hope by sharing it, people can learn a bit and get more ideas of what to do (and what not to do) for their own asset pipelines. In this part 1 post, I will cover the overall asset pipeline, focusing on how the Converter is structured. In part 2 I will go over more specifics of how individual assets are converted (like how textures are composited and compressed).\nContents High-Level Goals Before Asset Conversion Asset Files Parsing .paf Files Scanning The Scene For Used Assets Referenced Assets Non-Inferable Assets Asset Conversion What Exactly Is A Converted Asset Is The Asset Is Out-Of-Date Asset Versioning Finally Converting The Asset Common Pitfall Parallelization Creating The Fastfiles Inter-Asset Dependencies + Asset Ordering Versioning Getting Assets In Engine A Few Final Remarks High-Level Goals It\u0026rsquo;s important to realize what you want most out of your asset pipeline. The entire reason I made one in the first place, is because slow iteration times make my brain short-circuit. There\u0026rsquo;s nothing more frustrating than making a tiny change and then needing to wait several seconds (or often minutes at real studios) to see if it worked. That\u0026rsquo;s why for me, the number one goal was to have fast load times. As for the other aspects:\nConvert Times: Medium priority for me, since it directly impacts iteration speed but happens less often (for me) than booting the game. If you\u0026rsquo;re in a studio with many people converting? I\u0026rsquo;d argue it should be extremely high priority, though it often seems to get pushed to the side. Runtime Performance and Quality: High priority. How much you do offline directly impacts how fast the engine can process and render things. I usually make the choice to favor high FPS, but if you find yourself annoyed at long convert times, then it can definitely be beneficial to sacrifice a little FPS for faster iteration times, especially in development builds. Disk Size: Low priority, since I\u0026rsquo;m just making small demos and not shipping a game. I\u0026rsquo;m actually really passionate about this for real studios (the Call of Duty download sizes make me want to cry) and I think compression is super cool. But for a hobby engine? It hardly matters until you\u0026rsquo;re about to ship. Ease of Adding New Assets and Asset Types: Low priority, since I just do small scenes, and rarely need to add new asset types after the initial set (textures, models, pipelines, scripts, etc). Useability: Medium-high priority. I wanted a simple and consistent interface to access assets, both in the engine C++ code, and in game scripts. Before Asset Conversion So how does this all work in my engine? Well, there are 3 main executables:\nEngine.exe: the game Converter.exe: runs before the game. Responsible for processing all of the assets that a scene will need when loaded in the Engine. It loads them, converts them all to binary, and groups them into binary packages. I call those packages \u0026ldquo;Fastfiles\u0026rdquo;, because that was what they were called in CoD, and I got used to it. ModelExporter.exe: responsible for taking source model files (.obj, .fbx, .gltf, etc) and converting them into a common model file format (.pmodel) that the Converter can then use. This could have all been part of the Converter, but parsing model files can be really involved and sometimes the output can need some cleanup before being used in your real pipeline. So, it was helpful to have as a separate executable that runs once on every model you download, and then never again. We\u0026rsquo;ll walk through from the beginning of the pipeline when the Converter starts up, to loading a fastfile in the Engine. The ModelExporter won\u0026rsquo;t be discussed further here.\nAsset Files In order to use an asset in Progression, it first has to be described in a Progression Asset File (.paf). These are just JSON text files which contain all of the info needed to load an asset. I don\u0026rsquo;t really recommend JSON in hindsight, but it is convenient to use something like json or markup languages that already have fast parsers written for them. Here is an example of a couple assets:\n[{ \u0026#34;Image\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;kloppenheim\u0026#34;, \u0026#34;equirectangularFilename\u0026#34;: \u0026#34;images/skies/kloppenheim_06_2k.exr\u0026#34;, \u0026#34;semantic\u0026#34;: \u0026#34;ENVIRONMENT_MAP\u0026#34; }}, { \u0026#34;Pipeline\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;frustum_cull_meshes\u0026#34;, \u0026#34;computeShader\u0026#34;: \u0026#34;frustum_cull_meshes.comp\u0026#34; }}] You can see that each asset first defines the AssetType (Image and Pipeline above). Every asset also needs to have a name. In my engine, the name is the GUID. Different asset types can share the same name, but not two assets within the same type. I know some people use hashes for GUIDs, but I find using plain text names extremely convenient for readability, searchability, and debugging. I highly recommend them. Beyond the name, the parameters are specific to the asset and are used to define how to load the asset. These JSON definitions are directly parsed into [AssetType]CreateInfo structures which look like this:\nstruct BaseAssetCreateInfo { string name; }; struct ModelCreateInfo : public BaseAssetCreateInfo { string filename; // relative path to pmodel file bool recalculateNormals = false; }; ... So the first thing the Converter does, is parse every single .paf file into a bunch of [AssetType]CreateInfo structs. This defines the list of assets that a scene can reference, and all the info needed to load them, if so.\nParsing .paf Files How exactly you go from JSON -\u0026gt; CreateInfo isn\u0026rsquo;t super important, so I\u0026rsquo;ll just cover the main points here. The full code for this can be seen here in asset_parer.hpp, asset_parser.cpp, and asset_file_database.cpp.\nJust like how each asset type defined an [AssetType]CreateInfo struct that derived from BaseAssetCreateInfo, each type also defines an [AssetType]Parser that derives from BaseAssetParser. This is just so we can hold the parsers in a single array BaseAssetParser* g_assetParsers[ASSET_TYPE_COUNT], and then call a virtual Parse function that parses the JSON and returns a filled out [AssetType]CreateInfo. The caveat is that there is one extra level of inheritance with a templated class, to actually allocate the specific CreateInfo type.\nOne interesting aspect to consider here is inheritance (or parenting) in the JSON itself. Say you spent a lot of time filling out a complicated asset with many parameters. Later on it turns out you need to make 3 more just like the first one, with only 1-2 parameters changed. You could copy and paste the original asset definition 3 times, or you could declare that the 3 new assets inherit from that original asset. That way every parameter is copied except the name, and you only have to specify new parameters instead of all of them. In Progression this is done by specifying a \u0026ldquo;parent\u0026rdquo; parameter in the JSON. Then in the code, it is used like this:\nstd::shared_ptr\u0026lt;BaseAssetCreateInfo\u0026gt; parentCreateInfo = nullptr; if ( value.HasMember( \u0026#34;parent\u0026#34; ) ) parentCreateInfo = FindAssetInfo( assetType, value[\u0026#34;parent\u0026#34;].GetString() ); std::shared_ptr\u0026lt;BaseAssetCreateInfo\u0026gt; info; info = g_assetParsers[assetType]-\u0026gt;Parse( value, parentCreateInfo ); ... ... virtual BaseInfoPtr Parse( const rapidjson::Value\u0026amp; value, ConstBaseInfoPtr parentCreateInfo ) override { // create the specific [AssetType]CreateInfo, not a BaseAssetCreateInfo auto info = std::make_shared\u0026lt;DerivedInfo\u0026gt;(); // if this asset has a parent, copy all of the parameters except the name if ( parentCreateInfo ) *info = *std::static_pointer_cast\u0026lt;const DerivedInfo\u0026gt;( parentCreateInfo ); const std::string assetName = value[\u0026#34;name\u0026#34;].GetString(); info-\u0026gt;name = assetName; // finally, fill out the createInfo data by parsing the JSON return ParseInternal( value, info ) ? info : nullptr; } The caveat here, is that I only do a single-pass over the asset files. So, if an asset wants to use a parent, that parent must be defined earlier in the same asset file. I personally don\u0026rsquo;t mind this restriction, and I even think it helps keep things more contained. I definitely remember artists complaining about this at SHG though, which also used single pass asset parsing.\nScanning The Scene For Used Assets Now that we have the possible assets parsed and ready, the next step is to actually figure out which of those are needed for the scene. In my pipeline, the Converter is run per-scene like so: Converter.exe [sceneName]. My scene files are also JSON:\n[{ \u0026#34;Camera\u0026#34;: { \u0026#34;position\u0026#34;: [ -15, -25, 0 ], \u0026#34;rotation\u0026#34;: [ 0, 0, 0 ], \u0026#34;nearPlane\u0026#34;: 0.02 }}, { \u0026#34;Skybox\u0026#34;: \u0026#34;kloppenheim\u0026#34; }, { \u0026#34;Script\u0026#34;: \u0026#34;cameraController\u0026#34; }, { \u0026#34;DirectionalLight\u0026#34;: { \u0026#34;color\u0026#34;: [ 1, 1, 1 ], \u0026#34;direction\u0026#34;: [ 0, 0, -1 ] } }, { \u0026#34;Entity\u0026#34;: { \u0026#34;NameComponent\u0026#34;: \u0026#34;dragon\u0026#34;, \u0026#34;Transform\u0026#34;: { \u0026#34;position\u0026#34;: [ 3, 0, 0 ], \u0026#34;rotation\u0026#34;: [ 90, 0, 90 ], \u0026#34;scale\u0026#34;: [ 1, 1, 1 ] }, \u0026#34;ModelRenderer\u0026#34;: { \u0026#34;model\u0026#34;: \u0026#34;dragon\u0026#34;, \u0026#34;material\u0026#34;: \u0026#34;blue\u0026#34; } }}] So in the example scene above, the converter would need to convert: the image \u0026lsquo;kloppenheim, the script \u0026lsquo;cameraController\u0026rsquo;, the model \u0026lsquo;dragon\u0026rsquo;, and the material \u0026lsquo;blue\u0026rsquo;.\nReferenced Assets Assets can also implicitly reference other ones, so once the scene is parsed we call AddReferencedAssets on each of these assets. For example:\nvoid GfxImageConverter::AddReferencedAssetsInternal( ConstDerivedInfoPtr\u0026amp; imageInfo ) { if ( imageInfo-\u0026gt;semantic == GfxImageSemantic::ENVIRONMENT_MAP ) { auto irradianceInfo = std::make_shared\u0026lt;GfxImageCreateInfo\u0026gt;( *imageInfo ); irradianceInfo-\u0026gt;name = imageInfo-\u0026gt;name + \u0026#34;_irradiance\u0026#34;; irradianceInfo-\u0026gt;semantic = GfxImageSemantic::ENVIRONMENT_MAP_IRRADIANCE; AddUsedAsset( ASSET_TYPE_GFX_IMAGE, irradianceInfo ); auto reflectionProbeInfo = std::make_shared\u0026lt;GfxImageCreateInfo\u0026gt;( *imageInfo ); reflectionProbeInfo-\u0026gt;name = imageInfo-\u0026gt;name + \u0026#34;_reflectionProbe\u0026#34;; reflectionProbeInfo-\u0026gt;semantic = GfxImageSemantic::ENVIRONMENT_MAP_REFLECTION_PROBE; AddUsedAsset( ASSET_TYPE_GFX_IMAGE, reflectionProbeInfo ); } } You can see for images that are environment maps, we also generate two additional images: the irradiance map, and the reflection probe. These are later used for IBL lighting by the renderer. This can be used by any asset type. Pipeline assets add the individual shaders that they reference, for example.\nNon-Inferable Assets While parsing the scene this way gets most of the assets you need, what about assets that your Lua scripts might try to load? The only way to handle these is to explicitly make a list of assets that might be used by the script. For Progression, scene files are stored as [sceneName].json. But the Converter also checks to see if there exists a corresponding [sceneName].csv file in the same directory when processing a scene. If it does, then it loads this file in addition to the .json one. These files are simply lists of assets, in the form [AssetType],[AssetName] on each line.\nOne other area of non-inferable assets is scene-agnostic assets that are not tied to any game objects. One example of this would be compute shaders. These are also handled by adding them to .csv files like assets in scripts, but since these .csvs are agnostic to real scenes, they are stored in a special directory: assets/scenes/required/. When the Converter runs it processes every file in this directory, to always make sure the required assets are up to date and available. The Engine then usually manually loads these fastfiles at startup, like how the renderer loads the gfx_required fastfile to get all of the shaders it needs.\nAsset Conversion Now that we have the list of assets the scene uses, and all of their CreateInfo\u0026rsquo;s, it\u0026rsquo;s time to actually convert them. If you want to see the full code, look at ConvertAssets() in converter_main.cpp, all of base_asset_converter.hpp, and at each asset type\u0026rsquo;s converter. Just like the BaseAssetCreateInfo and BaseAssetParser pattern, there is also a BaseAssetConverter class:\nusing ConstBaseCreateInfoPtr = const std::shared_ptr\u0026lt;const BaseAssetCreateInfo\u0026gt;; class BaseAssetConverter { public: const AssetType assetType; BaseAssetConverter( AssetType inAssetType ) : assetType( inAssetType ) {} virtual ~BaseAssetConverter() = default; virtual string GetCacheName( ConstBaseCreateInfoPtr\u0026amp; baseInfo ) { return \u0026#34;\u0026#34;; } virtual AssetStatus IsAssetOutOfDate( ConstBaseCreateInfoPtr\u0026amp; baseInfo ) { return AssetStatus::UP_TO_DATE; } virtual bool Convert( ConstBaseCreateInfoPtr\u0026amp; baseInfo ) { return true; } virtual void AddReferencedAssets( ConstBaseCreateInfoPtr\u0026amp; baseInfo ) {} }; And just like the BaseAssetParser, there is also one extra intermediate base class, to handle the type casting to specic [AssetType]CreateInfo and any type-agnostic conversion code.\ntemplate \u0026lt;typename DerivedAsset, typename DerivedInfo\u0026gt; class BaseAssetConverterTemplate : public BaseAssetConverter { ... What Exactly Is A Converted Asset We haven\u0026rsquo;t actually covered what it means for an asset to be converted yet. In Progression, a converted asset is one that has been loaded, serialized to binary, and saved to a file. Specifically, these files all get saved under the asset cache directory, which is located at [projectDir]/assets/cache/. The filenames all take the pattern [assetName]_[createInfoHash]_[versionNumber].ffi, where .ffi stands for \u0026ldquo;fastfile intermediate\u0026rdquo;. For example, my assets/cache/models/ directory currently looks like this:\ncube_5919176923328749623_v6.ffi dragon_17433001533983433154_v6.ffi sponza_vulkansamples_15064524848871577573_v6.ffi I refer to the [assetName]_[createInfoHash] component as the \u0026lsquo;cache name\u0026rsquo;, and this is what the BaseAssetConverter::GetCacheName function returns. You don\u0026rsquo;t have to use this naming convention exactly, but I am relying on the fact that any changes to the asset\u0026rsquo;s CreateInfo data will change the cache name. A couple of naming alternatives might be:\nJust include the asset\u0026rsquo;s name right into the hash. This would give shorter and more consistent filenames, but I find having the asset name prefix makes for easier debugging when anything goes wrong in the Converter. Don\u0026rsquo;t hash everything, but rather just convert (some or all of) the CreateInfo data directly to a string. For example, if you had an asset that just had a dozen bools, you could just append a 1 or 0 to the cache name for each of the bools: assetName_100011110001_v0.ffi. This is nice because you can fully identify the entire CreateInfo just by looking at the cache name, which makes for more powerful debugging. However, I find most real asset CreateInfos have either a lot of parameters or long string parameters like filenames. As a result, using this style would make the cache name super long and unfeasible. Is The Asset Is Out-Of-Date We only want to actually convert an asset if it\u0026rsquo;s out-of-date. This is the job of the BaseAssetConverter::IsAssetOutOfDate function. There are two components to this:\nHas the asset been converted before? This one is assetType-agnostic and easy: just get the cache name for the asset and check to see if that cache file (.ffi) exists! This is why I said I am \u0026ldquo;relying on the fact that any changes to the asset\u0026rsquo;s CreateInfo data will change what the cache name.\u0026rdquo; It gives us a quick way to see if we\u0026rsquo;ve ever converted a given combination of settings for an asset before, because if it hasn\u0026rsquo;t, then no matching file will be found. If it has been converted before, did any of the asset\u0026rsquo;s source data change? This one depends on the asset type. For example, if we are converting a model asset whose source file is \u0026lsquo;dragon.obj\u0026rsquo;, then we need to check the timestamp on that .obj file. If the timestamp is newer than our cache file\u0026rsquo;s timestamp, then we need to reconvert. For images, you would have to check the source .png file(s) (possibly multiple, for cubemaps) instead. Asset Versioning Assets can change over time. How many parameters they have, their values, how they\u0026rsquo;re serialized, etc. When this happens, it naturally changes what the converted assets binary would be as well. This creates a potential problem: if we update how a model asset is converted, then we need to mark every single model as out-of-date regardless of what the timestamps are. The way this is done is through asset version numbers. Each asset type has a version number, and when you change how an asset is converted, you bump the version number for that asset type. Since these version numbers are included in the cache filename, bumping the version number always causes those assets to be considered out-of-date.\nFinally Converting The Asset Just like all the other classes before this, I use a BaseAsset virtual class, that all the real asset types inherit from:\nclass BaseAsset { public: BaseAsset() = default; virtual ~BaseAsset(); virtual bool Load( const BaseAssetCreateInfo* baseInfo ) { return false; } virtual bool FastfileLoad( Serializer* serializer ) = 0; virtual bool FastfileSave( Serializer* serializer ) const = 0; virtual void Free() {} ... }; A brief explanation of these functions:\nLoad: takes in the CreateInfo and is expected to load the asset from source. Used by the Converter, but compiled out of the Engine, since the Engine should always be loading converted assets, not source assets. FastfileLoad: loads a binary converted asset. Used by the Engine, not the Converter. FastfileSave: serializes a converted asset. Used by the Converter, not the Engine. Free: Engine only, mainly used for freeing up gpu resources the asset might have. So, in the Converter all we really to do with out-of-date assets is call Load with the appropriate CreateInfo, and then FastfileSave:\nvirtual bool ConvertInternal( ConstDerivedInfoPtr\u0026amp; derivedCreateInfo ) { DerivedAsset asset; const std::string cacheName = GetCacheName( derivedCreateInfo ); asset.cacheName = cacheName; if ( !asset.Load( derivedCreateInfo.get() ) ) { LOG_ERR( \u0026#34;Failed to convert asset %s %s\u0026#34;, g_assetNames[assetType], derivedCreateInfo-\u0026gt;name.c_str() ); return false; } if ( !AssetCache::CacheAsset( assetType, cacheName, \u0026amp;asset ) ) { LOG_ERR( \u0026#34;Failed to cache asset %s %s (%s)\u0026#34;, g_assetNames[assetType], derivedCreateInfo-\u0026gt;name.c_str(), asset.cacheName.c_str() ); return false; } return true; } The AssetCache::CacheAsset simply opens the appropriate .ffi file, and then calls FastfileSave to serialize the asset\u0026rsquo;s data into the file.\nCommon Pitfall Sometimes, when you\u0026rsquo;re adding a new asset, you mess up and have some bugs in either Load or FastfileSave. If the bug causes the Converter to crash while saving the .ffi file, this causes an issue. When you fix the bugs and go to run the Converter again, it will see that the .ffi exists, with a brand new timestamp, and think the asset is up to date, even though it was only a partially written file before the crash! The only way to fix the issue at that point, would be to either force convert the asset that failed, or delete the invalid .ffi file.\nThe way I try to mitigate this happening is first by not opening the .ffi until Load has fully finished. Second, I wrap the call to FastfileSave in a try/catch block, and if there is an exception I delete the file in the catch block. This seems to work reasonably well, though I think a better way might just be to write to a temporary .ffi file, and then if everything succeeds, rename that file to the intended cache name. That way, if your computer blue screens or you lose power while an asset is being serialized, you could just run the Converter again like normal.\nParallelization With the system described so far, scenes need to be processed one at a time, loaded single-threaded. For my engine, that\u0026rsquo;s really not an issue, since it\u0026rsquo;s very quick. The actual asset conversion is the important piece to parallelize. Fortunately, we\u0026rsquo;ve set things up so that each asset can be processed independently, all they need is their CreateInfos. So, I just get the list of all the out-of-date assets, and then convert them all in parallel using OMP. This works great, though the one caveat is that by default OMP doesn\u0026rsquo;t allow nested parallel calls. So if any of your asset Load functions are parallelized the same way, you can consider adding omp_set_nested( 1 ); to the start of the Converter to allow nested parallelization. I found this helpful because my environment maps are pretty slow to process, so the converter would stall waiting for that to finish without nested parallelization.\nCreating The Fastfiles At this point in the Converter, every asset has been converted and is up to date, so it\u0026rsquo;s time to create the fastfile. I haven\u0026rsquo;t mentioned what the fastfile actually is yet though: it\u0026rsquo;s simply all of the converted assets bundled into one file. This is to make load times faster by just having a single file IO call that can be read serially, instead of a ton of file IO by using the .ffi files directly. You can add whatever metadata you want, but currently, my fastfiles are literally just lists of [AssetType][AssetBinary] pairs.\nNow the question is: do we need to rebuild the fastfile? Well, it\u0026rsquo;s very similar to checking for out-of-date assets, with one extra case:\nDoes the fastfile exist? I store all of mine in the asset/cache/fastfiles/ with the naming convension [sceneName]_[version].ff. If this file is not found, the fastfile must be built. If the fastfile does exist, then we need to compare the fastfile\u0026rsquo;s timestamp, to every single asset used in the scene. If any of the asset timestamps are newer than the fastfile, it means the fastfile is out-of-date. It\u0026rsquo;s not enough to just check if ( numberOfOutOfDateAssets \u0026gt; 0 ), because the assets could have been converted from a different scene, but are still newer than the current scene\u0026rsquo;s fastfile. Finally, we need to check if the list of assets we would put in the fastfile if we build it, is different than the list of assets that are already in the previously built fastfile. To track this, every time a fastfile is built I export a text file of [AssetType],[AssetName] pairs for every asset used in that fastfile, and store it in asset/cache/assetlists/[sceneName].txt. The next time the converter is run, we can compare the current list of assets to the ones in this textfile. If the lists differ at all, the fastfile must be rebuilt. Inter-Asset Dependencies + Asset Ordering Sometimes assets reference other assets, like a material referencing its albedo and normal textures. So, when we actually deserialize these assets in Engine, we either need to make sure the images were already loaded before the materials get loaded, or do two-pass loading to fixup the references. I chose the first option for simplicity. This means, however, that the order of assets in our fastfile matters. I do this by grouping the assets by type, and having an explicit type order:\nenum AssetType : u8 { ASSET_TYPE_GFX_IMAGE = 0, ASSET_TYPE_MATERIAL = 1, ASSET_TYPE_SCRIPT = 2, ASSET_TYPE_MODEL = 3, etc... }; As you can see, all of the images are first. That\u0026rsquo;s because they don\u0026rsquo;t reference anything else. Materials need to go after images because they reference images. Scripts don\u0026rsquo;t reference anything, so they can be ordered anywhere, but models can reference materials, so they have to be after materials.\nVersioning Just like converted assets, we have to consider when we make changes to the converter. If we change the version number on any of the assets, we have to rebuild the fastfile. We also need to have a separate version number just for fastfiles for when the fastfile serialization or metadata is changed, independent of converted assets. In Progression it looks like this:\nconstexpr i32 g_assetVersions[] = { 9, // ASSET_TYPE_GFX_IMAGE, \u0026#34;New name serialization\u0026#34; 10, // ASSET_TYPE_MATERIAL, \u0026#34;New name serialization\u0026#34; 1, // ASSET_TYPE_SCRIPT, \u0026#34;New name serialization\u0026#34; 6, // ASSET_TYPE_MODEL, \u0026#34;Add meshlet cull data\u0026#34; etc.... }; constexpr u32 PG_FASTFILE_VERSION = 10 + ARRAY_SUM( g_assetVersions ); // reason The comments are intentionally there so that if two people made different changes and had to bump the same version number, there would be a merge conflict. Without the comment, it would auto-merge to only bump the number by one, even though two changes actually happened.\nOne note here, is that I append the version number to the fastfile\u0026rsquo;s name. This works, but anytime it gets bumped the old fastfiles just lay around and take up space. I think it\u0026rsquo;s a better idea to serialize the version number into the fastfile, and then check to see in Engine if the version number matches what it expects. This would keep the cache directory smaller when version bumps happen.\nGetting Assets In Engine In Progression, the AssetManager handles the loading and storing of every asset. See asset_manager.hpp and asset_manager.cpp for the full code. This is where the LoadFastFile function is implemented, and it\u0026rsquo;s very simple: memory map the fastfile, look at the first asset type, allocate it + call its FastfileLoad, and then move on to the next asset and repeat. There is a hash map per AssetType for storing these:\nunordered_map\u0026lt;string, BaseAsset*\u0026gt; g_resourceMaps[ASSET_TYPE_COUNT]; You can see that I\u0026rsquo;m just using plain pointers, but you probably want your loaded assets to be ref-counted. I haven\u0026rsquo;t added this yet, but mostly because I load one demo scene at a time, and it doesn\u0026rsquo;t really matter :) If you ever manage multiple scenes though, when you unload a scene, you will want to free the assets that are not shared by any other scene.\nAs for accessing these loaded assets, I had decided early on that I wanted to have the interface AssetManager::Get\u0026lt;AssetType\u0026gt;( assetName ). This just seemed nice and simple to me, which is one of the goals I had. An example would be Material* mat = AssetManager::Get\u0026lt;Material\u0026gt;( \u0026quot;wood_floor\u0026quot; );. To make this happen, we need a way to convert from actual template AssetType to its ASSET_TYPE enum value, to appropriately index into g_resourceMaps[ASSET_TYPE_COUNT]. The only way I know how to do that in C++, is to use static variables:\nstruct GetAssetTypeIDHelper { static u32 IDCounter; }; template \u0026lt;typename Derived\u0026gt; struct GetAssetTypeID : public GetAssetTypeIDHelper { static u32 ID() { static u32 id = IDCounter++; return id; } }; We can then do things like GetAssetTypeID\u0026lt;Material\u0026gt;::ID() to get an index from a type. The caveat is that we have to initialize these in the same order their type appears in the ASSET_TYPE enum, which I do at the beginning of AssetManager::Init:\nvoid Init() { GetAssetTypeID\u0026lt;GfxImage\u0026gt;::ID(); // ASSET_TYPE_GFX_IMAGE GetAssetTypeID\u0026lt;Material\u0026gt;::ID(); // ASSET_TYPE_MATERIAL GetAssetTypeID\u0026lt;Script\u0026gt;::ID(); // ASSET_TYPE_SCRIPT etc... And with that, we\u0026rsquo;ve covered the entire asset pipeline from start to finish! I hope that gives some more insight into how these pipelines work.\nA Few Final Remarks There are a few final things I\u0026rsquo;d like to discuss about the whole pipeline, particularly in regards to the high-level goals we initially set out to achieve:\nPerformance: The main goal was to have fast load times. Did we succeed? I\u0026rsquo;d say yes! Here are the load times for two scenes:\nCrytek Sponza: 41ms, for a 77MB fastfile containing 77 assets Intel Sponza: 428ms, for a 1.212GB fastfile containing 86 assets (has much larger models and 4k textures compared to Crytek\u0026rsquo;s). I do have an NVME SSD which helps a lot, but I\u0026rsquo;m still quite pleased with the load times. The load times also include creating and uploading the textures + models to the GPU. There was also a secondary goal of decent Converter performance. Now this one heavily depends on what scene you are converting, but overall, my convert times are OK, but not great. For example, a fully fresh convert of Crytek\u0026rsquo;s Sponza takes 2.5 seconds on my machine, while the Intel Sponza (plus a big skybox) takes 36 seconds. However, the second convert once nothing has changed is only 21ms. This is largely because I haven\u0026rsquo;t taken the time to optimize convert times, and for a hobby engine kind of lean towards slow but simple converters compared to a lot of complicated fast paths.\nHot Reloading: Currently, my engine doesn\u0026rsquo;t support hot reloading of assets. It did, a long long time ago, but it was implemented awkwardly, so I ripped it out. It\u0026rsquo;s something I\u0026rsquo;d definitely like to add again one day, but honestly for Progression? It doesn\u0026rsquo;t add a lot of value, when booting the Engine and loading a Scene takes less than one second. The iteration time is already very low :)\nBreakage Frequency and Debugging: Surprisingly, this breaks less often than you\u0026rsquo;d think! It is fairly easy to add bugs, especially with custom serialization and deserialization. However, I find that with the pipeline\u0026rsquo;s simple structure and naming conventions, it rarely takes me a long time to figure out what mistake I made. In my mind, as long as bugs don\u0026rsquo;t happen too often and are quick to fix, then you\u0026rsquo;ve succeeded at something.\nDisk Size + Compression: This is a huge one that I haven\u0026rsquo;t talked about yet. Any real engine will compress their assets/fastfiles/packages, typically in a format that gives very fast decompression rates (LZ4, Oodle, etc). Progression currently doesn\u0026rsquo;t compress anything however. I would like to add it of course, but part of the reason I haven\u0026rsquo;t bothered yet, is because of some small experiments with LZ4. Since I haven\u0026rsquo;t played around with RDO on my textures, using LZ4 on them usually only saves between 0-2%. Even on the rest of the assets, the savings are not amazing. For example, here are the results of using LZ4 on the entire fastfile for the two Sponza scenes I mentioned above:\nCrytek Sponza: 14.1% savings with default compression, and 20.9% savings with LZ4_HC Intel Sponza: 11% savings with default compression, 18% with LZ4_HC Not really substantial enough to make me add it yet, especially since LZ4_HC isn\u0026rsquo;t super fast. I\u0026rsquo;d rather just keep the fastest iteration times for now. I do love compression, however, so I definitely want to revisit this at some point, and try to use RDO and make assets compression-friendly :)\nAnd with that, I think I\u0026rsquo;ve covered everything I wanted to. I\u0026rsquo;ll definitely post a part 2, going over how specific assets are converted, but I hope the general pipeline structure makes sense, and why I chose to design it like that. Thanks to everyone who stuck around to read the whole thing, this definitely got longer than I thought it would. Leave a comment if you\u0026rsquo;d like; I\u0026rsquo;d love to hear feedback, and love to hear other decisions people made when structuring their asset pipelines!\n","permalink":"https://liamtyler.github.io/posts/asset_pipeline_part_1/","summary":"Asset pipelines are one of the pieces of game development I don\u0026rsquo;t see people talk about very often. The main concepts are often introduced and chatted about theoretically, sure: \u0026ldquo;convert your assets into binary packages, compress them, and load them at runtime,\u0026rdquo; but rarely do I find people chatting about the implementation details. It always strikes me as a little odd, because I\u0026rsquo;d argue it has the single biggest impact on development.","title":"Asset Pipelines Part 1"},{"content":"My name is Liam Tyler. I\u0026rsquo;ve been doing graphics for about 7 years. In terms of work, I was previously a senior rendering engineer at Deviation Games working on an unnanounced UE5 game. Before that, I was a rendering engineer at Sledgehammer Games working on the Call of Duty games. I\u0026rsquo;m currently taking a small sabbatical from work to focus on my health and to hike a bunch of mountains in Colorado this summer.\nThis blog is mostly going to be a combination of various graphics or coding things that I\u0026rsquo;ve found interesting, as well as learnings from working on my little hobby engine.\nYou can contact me at Tyler.Liam7@gmail.com.\n","permalink":"https://liamtyler.github.io/about/","summary":"My name is Liam Tyler. I\u0026rsquo;ve been doing graphics for about 7 years. In terms of work, I was previously a senior rendering engineer at Deviation Games working on an unnanounced UE5 game. Before that, I was a rendering engineer at Sledgehammer Games working on the Call of Duty games. I\u0026rsquo;m currently taking a small sabbatical from work to focus on my health and to hike a bunch of mountains in Colorado this summer.","title":"About Me"},{"content":" In this post, I want to explain how to take a normal map and generate its corresponding height map (also called displacement map). There are multiple ways to achieve this, but the one I\u0026rsquo;ll be covering in this post is the method that Danny Chan presented in Advances in Real Time Rendering at Siggraph 2018 (link here, page 16). It\u0026rsquo;s also the same method that the Call of Duty engine still uses today to generate the vast majority of its height maps. All the code I\u0026rsquo;ll cover in this post is implemented here.\nWhy There are several reasons why having this process can be helpful:\nThe simplest one: very often no height map is available. Even if you aren\u0026rsquo;t going to actually displace the surface, height maps are useful for a number of reasons, such as blending, or generating quality AO as described in Danny\u0026rsquo;s presentation linked above (pages 16-22). Even if there is a height map, it\u0026rsquo;s often \u0026lsquo;wrong\u0026rsquo;. What I mean, is that the slopes of the surface implied by the height map often don\u0026rsquo;t match the slopes in the normal map. A height map and normal map are just two different ways of representing the exact same surface, and they are intrinsically linked together. For example, say you\u0026rsquo;re displacing the ground, and it creates a small 45-degree hill. If your normal map returns a slope of 80 degrees, then you\u0026rsquo;ll be lighting the hill as if it should be steeper, and it will just look off:\nThe majority of painted or edited height maps just won\u0026rsquo;t have this guarantee of matching the normals (or vice versa), and therefore be wrong. While the above image could just have either the heights or normals scaled to fix the mismatch, it\u0026rsquo;s also common with painted normal maps where they end up being \u0026lsquo;impossible\u0026rsquo; (there exists no surface that could match the slopes). In general, normal maps that are captured through photogrammetry are the best ones. However, even if you\u0026rsquo;re using photogrammetry assets, the height map provided with them is often cleaned up or scaled to look \u0026lsquo;pretty good\u0026rsquo;, but not actually made to match the normal map very well (I\u0026rsquo;ll show an example of this later). Nearly all engines provide two extra parameters when doing displacement: displacement scale and bias. In most of those, changing the displacement scale causes the geometric and normal slopes get out of sync. This is because usually the displacement height and normals are calculated completely independently from each other in the shader:\ndisplacementHeight = scale * sampledHeightMapValue + bias;\ndisplacedNormal = regularNormalMappedNormal;\nReally though, changing the displacement scale changes the slope of the geometry, so the normal should be scaled as well. You can play around with the displacement bias as much as you want since that just moves the whole surface, but changes to the scale mean the normals need to be scaled as well. The reverse is true as well: scaling the normals means you need to change the height map or displacement scale too. Because of reasons #2 and #3 above, the Call of Duty pipeline actually doesn\u0026rsquo;t allow you to use a hand-authored height map and a normal map at the same time on a material. You can either:\nProvide only a normal map and have the height map auto-generated from the normal map using the method I\u0026rsquo;ll describe in the rest of this article. Provide only a height map and have the normal map auto-generated from the height map. In both cases, it ensures that the height map and normals will always match. A follow-up question is then \u0026ldquo;what about the displacement scale\u0026rdquo;? The scale is no longer tied to the material, but the textures themselves:\nIf using a normal map, a new float parameter is introduced to scale the normals. The normals will be scaled before generating the height map. If using a height map, then you can simply scale the heights before generating the normal map. In both cases again, the maps will be matching. Now that that\u0026rsquo;s all explained, let\u0026rsquo;s dive into how this normal-to-height generation actually works.\nHow it works Core Idea The algorithm itself is actually pretty short. The core idea is that for any pixel, we can estimate its height given the heights and slopes of its neighbors. Take the image below for example, where we want to estimate the height of the center pixel '?'. Since we have the normal map as input, we know the slope (height derivative) in the X and Y directions for each pixel. So one guess for the middle pixel\u0026rsquo;s height would be L + Slope(L).x. Another guess would be D - Slope(D).y (note that we are assuming that normals the normals are +Y down, and +X to the right, but you can flip the signs if your normal maps are different). We can do this for all 4 neighbors, and take the average to get a pretty good guess for the middle\u0026rsquo;s height.\nYou might be confused though, \u0026ldquo;we don\u0026rsquo;t know the neighbor\u0026rsquo;s heights yet\u0026rdquo;? Correct, but it doesn\u0026rsquo;t matter! The cool thing about this algorithm is that if you just make an initial guess for the heights, and do this neighbor guess averaging over and over, it will eventually converge! So if we start with a guess of all heights = 0 and iterate a bunch of times, it converges to the final height map we want. Pretty neat, right? There are some other details and tricks to speed up the convergence though, so let\u0026rsquo;s get into the specifics.\nImplementation So the first part of the algorithm is to convert the normals into slope space. This is just the partial derivatives of the height field with respect to X and Y. Aka, how much the height changes in the X and Y directions: \\[ \\frac{\\partial z}{\\partial x} = \\frac{-n.x}{n.z} \\] \\[ \\frac{\\partial z}{\\partial y} = \\frac{-n.y}{n.z} \\] vec2 DxDyFromNormal( vec3 normal ) { vec2 dxdy = vec2( 0.0f ); if ( normal.z \u0026gt;= 0.001f ) dxdy = vec2( -normal.x / normal.z, -normal.y / normal.z ); return dxdy; } We do this for every pixel in the normal map and divide by the input texture size. This is just to normalize the slopes to be independent of the texture size. Without it, the height map would get taller as you increased the normal map size for example.\nFloatImage2D dxdyImg = FloatImage2D( width, height, 2 ); const vec2 invSize = { 1.0f / width, 1.0f / height }; for ( int i = 0; i \u0026lt; width * height; ++i ) { vec3 normal = normalMap.Get( i ); dxdyImg.Set( i, DxDyFromNormal( normal ) * invSize ); } The last thing to do before we get to the meat of the algorithm is to allocate two more images: the final output height map (last argument), and a scratch height map. The latter is just used internally by the algorithm during iteration, which we will see in a second.\nFloatImage2D scratchHeights = FloatImage2D( width, height, 1 ); FloatImage2D outputHeights = FloatImage2D( width, height, 1 ); BuildDisplacement( dxdyImg, scratchHeights.data.get(), finalHeights.data.get() ); Now the core of BuildDisplacement is the repeated averaging of the height guesses I explained earlier. This iteration method is also called relaxation. You could do this on the current resolution as-is, but it would take a long time to converge (often tens of thousands, or hundreds of thousands of iterations). As the presentation mentions, to speed up the convergence you can instead: recursively downsample the image, iterate on that a smaller number of times (tens, or hundreds of times, not thousands), and then upsample that to use as the starting guess for next round of iterations. Confusing? Here\u0026rsquo;s the code:\nvoid BuildDisplacement( const FloatImage2D\u0026amp; dxdyImg, float* scratchH, float* outputH ) { int width = dxdyImg.width; int height = dxdyImg.height; if ( width == 1 || height == 1 ) { memset( outputH, 0, width * height * sizeof( float ) ); return; } else { int halfW = Max( width / 2, 1 ); int halfH = Max( height / 2, 1 ); FloatImage2D halfDxDyImg = dxdyImg.Resize( halfW, halfH ); float scaleX = width / static_cast\u0026lt;float\u0026gt;( halfW ); float scaleY = height / static_cast\u0026lt;float\u0026gt;( halfH ); vec2 scales = vec2( scaleX, scaleY ); for ( int i = 0; i \u0026lt; halfW * halfH; ++i ) halfDxDyImg.Set( i, scales * halfDxDyImg.Get( i ) ); BuildDisplacement( halfDxDyImg, scratchH, outputH ); // upsample the lower resolution height map, outputH, into scratchH stbir_resize_float_generic( outputH, halfW, halfH, 0, scratchH, width, height, 0, 1, -1, 0, STBIR_EDGE_WRAP, STBIR_FILTER_BOX, STBIR_COLORSPACE_LINEAR, NULL ); } ... neighbor iteration ... Let\u0026rsquo;s walk through an example image of size 32x32. It would first downsample the slope image to 16x16: halfDxDyImg = dxdyImg.Resize( 16, 16 ). The next step is to update the slopes to account for the bigger pixel footprint. Remember we initially divided the slopes by the texture size: DxDyFromNormal( normal ) * invSize. So each slope in that image was describing \u0026ldquo;how much does the height change if we move 1/32nd of a unit\u0026rdquo;. Each texel in the lower resolution image now describes \u0026ldquo;how much does the height change if we move 1/16th of a unit\u0026rdquo;, so we need to double the slopes from the higher resolution image.\nNext, we call BuildDisplacement again until we get down to a 1x1 image. Here we seed the initial guess of all heights to 0. You can use something other than 0, the only thing it changes is where the average height of the final image is (moves the entire surface up/down). It will then upsample to 2x2 and do the neighbor iteration. Then upsample that to 4x4 and do the neighbor iteration again, then 8x8, etc. So how does the neighbor iteration look exactly?\nfloat* cur = scratchH; float* next = outputH; // ensure numIterations is odd, so the final output is stored in \u0026#39;next\u0026#39; aka outputH numIterations += numIterations % 2; for ( uint32_t iter = 0; iter \u0026lt; numIterations; ++iter ) { #pragma omp parallel for for ( int row = 0; row \u0026lt; height; ++row ) { int up = Wrap( row - 1, height ); int down = Wrap( row + 1, height ); for ( int col = 0; col \u0026lt; width; ++col ) { int left = Wrap( col - 1, width ); int right = Wrap( col + 1, width ); float h = 0; h += cur[left + row * width] + 0.5f * dxdyImg.Get( row, left ).x; h += cur[right + row * width] - 0.5f * dxdyImg.Get( row, right ).x; h += cur[col + up * width] + 0.5f * dxdyImg.Get( up, col ).y; h += cur[col + down * width] - 0.5f * dxdyImg.Get( down, col ).y; next[col + row * width] = h / 4; } } std::swap( cur, next ); } A few notes on this:\nThe neighbor averaging can\u0026rsquo;t be done in place (much like a blur filter), which is why it flips between scratchH and outputH each iteration. The code is written to assume that the final iteration will store into outputH, which means we must have an odd number of iterations (store results in outputH on iteration 1, scratchH on iteration 2, outputH on iteration 3, etc.). You can also see that I just made the assumption that all of these input textures wrap. If they can\u0026rsquo;t wrap, then you can try the usual tricks at the edges (clamp/reflect). To be honest, I haven\u0026rsquo;t tried it yet personally. All of the slopes are multiplied by 0.5. Embarrassingly, I have no idea why this is needed! I found out it gives much better results accidentally when I was trying to average the neighbor slopes with the center pixel\u0026rsquo;s slope (the center slopes cancel out anyways, but the 0.5 multiplier significantly improved things). Perhaps I missed a factor of 2 somewhere else, not sure. After that, the only remaining step is to remap the final image to be 0 - 1, for minimal loss when saving it out (if saving the result out as 8 or 16-bit). The scale needed for remapping (maxHeight - minHeight) is the same needed later in the shader for unpacking. The same goes for the bias (minHeight).\nThe Results It\u0026rsquo;s a bit of a challenge to quantify exactly how good the generated height maps are, particularly when we don\u0026rsquo;t have ground truth images to compare them to! The only ground truth images we have are the normal maps. Because of that, I took the generated height maps and generated normal maps from those. That way we can do direct comparisons and gather standard image diff metrics (MSE/PSNR). Now, there is a huge caveat here: both the normal-to-height and the height-to-normal processes introduce error. So by comparing the two normal maps, we are actually measuring the combined error of both processes, not just the first normal-to-height error that we would like. But it\u0026rsquo;s better than nothing, so here we go.\nBelow are the results of 5 test cases (3 photogrammetry, 2 synthetic). The first column is the starting ground truth normal map, the second column is the generated normal map, and the third column is the generated height map. There are many ways to generate normals from a height field, I chose the simplest one for this, just doing the finite difference between the left + right neighbors, and the up + down neighbors (search for GetNormalMapFromHeightMap in the code). All of these height maps were generated using 1024 iterations. To get the PSNR between the two normal maps, I generated the dot product between both images, adjusted so that 0 means both vectors are the same, and 2 would mean they are completely 180 from each other: 1 - Dot( n1, n2 ). I then found the MSE of that dot product image and converted that to PSNR (search for CompareNormalMaps in the code). Here were the results (click images to enhance and flip between them):\nPSNR = 29.7 PSNR = 30.2 PSNR = 31.4 PSNR = 72.9 PSNR = 50.2 Pretty good in my opinion! If you really squint, you can see where it doesn\u0026rsquo;t do so well. The last synthetic image is a nice example of how it doesn\u0026rsquo;t do well at sharp corners. Some of that is just due to the error in the height-to-normal process, but some of it also is from the height generation.\nCompared To Provided Height Map Remember when I said that sometimes even with photogrammetry, the provided height map just doesn\u0026rsquo;t match the normals that well? The 2nd rock image above is a good example of this.\nThis asset is from Poly Haven, which is wonderful and actually provides a blender file that lets us see what the author says the scale should be. In this case, it\u0026rsquo;s 0.20. However, look what happens when we generate the corresponding normal map, using the provided height map + 0.20 scale:\nPSNR = 20.6 It basically kept all of the flatter top surfaces accurate, but heavily increases all the other slopes. It also has a PSNR of 20.6. If we get rid of the 0.20 scale and do a search for the scale that makes for the best PSNR, we find that a scale of 0.065 gives us a PSNR of 23.5:\nPSNR = 23.5 Now we have the opposite problem: the steeper slopes and crevices were preserved better at the cost of flattening everything else. This means that really, there is no scale that will preserve both areas\u0026ndash; the map itself just doesn\u0026rsquo;t match the normals well. If we use the auto-generated height map, however, you can see that it matches a lot better. Not perfectly, but definitely more than the provided height map can:\nPSNR = 30.2 For the record, I\u0026rsquo;m not blaming the author of this material at all. This is a great asset to have, and I bet it still looks pretty good when displaced. But if your goal is to have the geometric and normal slopes match like we have been arguing it should, then the height map isn\u0026rsquo;t quite the best for it. So just a warning that just because a material was captured with photogrammetry doesn\u0026rsquo;t mean it\u0026rsquo;s always perfect.\nHow Long To Iterate? You might notice that I didn\u0026rsquo;t define numIterations anywhere in the code snippets above. I was glossing over that because it\u0026rsquo;s a bit trickier to answer! The longer you run this algorithm, the longer it has to converge. How do you know when it is done converging? There are a couple of ways you could define convergence and measure it. Since our initial guess was a flat surface, however, a pretty good way you can tell if it has converged is when the surface stops getting taller as you increase the number of iterations. Even for images with lots of crevices and details in the middle height ranges, tracking the total height still seems to be a good way of defining total convergence, at least with all of my test cases. Now, you can track the min and max height after each iteration to do this, but:\nThe height changes are very small per iteration and get increasingly smaller with each further iteration. That means you\u0026rsquo;ve really just swapped to a new, equally hard problem: what should the super small threshold be exactly? It\u0026rsquo;s slightly more annoying and slower to track each iteration since we are multi-threading. The option I took instead was to just try a bunch of different iteration counts on a number of images and see how they converged. Below is the results of 7 different normal maps, where the height map was generated at 9 different iteration counts: 32, 64, 128, 256, 512, 1k, 2k, 4k, and 32k. If we consider the scale given by the 32k image as \u0026lsquo;fully converged\u0026rsquo; we can then divide all of the other scales (per image) by that to normalize everything:\nYou can see that there is some slight variation between normal maps, but they all pretty closely follow the same trend. Basically everything above 2k is imperceptible, and 512-1024 iterations are probably more than enough for the vast majority of images. I default to 512 as being \u0026lsquo;good enough\u0026rsquo;, but you can pick whatever iteration count you feel is most appropriate for your application. For many applications, sub-100 iteration counts can be perfectly fine. I will say that if you are using these as displacement maps though, I have noticed iteration counts under 100 can sometimes (not always) be noticeably not done converging, like where sections of geometry that should be straight are actually curved because it didn\u0026rsquo;t converge yet.\nCan We Make The Algorithm Faster? There are a few obvious ways this code could be improved: the parallelization could be better, the downscaling of the dxdy image and upscaling of the height image could for sure be sped up (I was just using the easiest stb option for them), and you could likely do more intelligent ways of calculating numIterations. Implementing those isn\u0026rsquo;t the point of this post though, the question I want to ask here is: can the algorithm itself be tweaked to be faster? Turns out, the answer is yes!\nTake one of the rock textures from above, which has size 1024x1024. If we save out all of the intermediate height maps (what BuildDisplacement returns) and scale them up to 1024x1024 for comparison, we get the following (sizes 8x8 to 1024x1024):\nYou can see that pattern: as the mips get larger, the less impact each set of iterations has. This makes sense\u0026ndash; you would hope that once all of the iterations on the 1x1 -\u0026gt; 128x128 sized mips are done that all of the details sized \u0026gt;= 1/128th of the image would be done converging. So, there should be less work to do as the image gets larger, because the larger iterations have better and better initial guesses and will mostly just be converging the smaller and less noticeable details.\nSo what if we reduced how many iterations just the largest mips got? To do this, let\u0026rsquo;s introduce an iteration multiplier. It will scale the number of iterations of mip 0 by that amount, and then we can double the multiplier for the next mip. This way we have fewer iterations on the larger mips, but the same amount on the smaller ones:\nvoid BuildDisplacement( const FloatImage2D\u0026amp; dxdyImg, float* scratchH, float* outputH, uint32_t numIterations, float iterationMultiplier ) {\t... BuildDisplacement( halfDxDyImg, scratchH, outputH, numIterations, 2 * iterationMultiplier ); ... numIterations = (uint32_t)(Min( 1.0f, iterationMultiplier ) * numIterations); numIterations += numIterations % 2; // ensure odd number ... The results? If we do 1024 iterations with a 0.25 multiplier on the rocky image above, we get nearly the exact same quality as a 512 iteration with a 1.0 multiplier, but 25% faster. What if the image was larger though? We are predicting that those details matter less and less. It turns out the prediction is true! Doing 1024 iterations with a 0.25 multiplier on a 2048 version of this same image gives the same quality as doing 710 iterations, but again faster than 512. If we do the same with a 4096 version of the image, it gets even better: the same quality as doing 794 iterations, but the same cost as doing 350 iterations!\nSo what multiplier should you use? Like the number of iterations, it just depends on what quality you want and how fast you want it. Remember this also works better on larger images like 2k or 4k. I like to default to 1024 iterations with 0.25 multiplier, especially with 1k+ images, but adjust it as you see fit!\nFuture Work While this method works well (don\u0026rsquo;t forget it\u0026rsquo;s used in AAA games like Call of Duty!), there are a lot of questions and issues we ignored:\nSince the height algorithm always pulls from its neighbors, it\u0026rsquo;s guaranteed to mess up (smooth) sharp corners Similarly, the way we reconstructed the normals increases how much error there is at sharp corners, making it harder to examine the error from each step Explaining why as you increase the iteration count, the PSNR almost always goes down slightly If you zoom in and compare the original and generated normal maps, you\u0026rsquo;ll notice the generated one has most of the high frequency details blurred out Are there alternative ways to generate the height maps that might be better or faster? And more. I think this post is definitely long enough as-is, however, so I\u0026rsquo;m going to write a part 2 where I dig deeper into these issues and try to answer some of these questions. Once that is posted, I\u0026rsquo;ll update this page with a link to it.\nCredits Huge thanks to the providers of the source normal maps (list of each one, and links to them can be found here.\n","permalink":"https://liamtyler.github.io/posts/normal-to-height/","summary":"In this post, I want to explain how to take a normal map and generate its corresponding height map (also called displacement map). There are multiple ways to achieve this, but the one I\u0026rsquo;ll be covering in this post is the method that Danny Chan presented in Advances in Real Time Rendering at Siggraph 2018 (link here, page 16). It\u0026rsquo;s also the same method that the Call of Duty engine still uses today to generate the vast majority of its height maps.","title":"Generating Height Maps From Normal Maps"}]